{
  "hash": "8b70db0e5cdb5ea6cd55c8957dae2897",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Prediction\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: source\n---\n\n\n## Overview of today\n\n1.  Training and Testing\n2.  Cross-validation\n3.  Statistical Prediction Algorithms\n\n**Goal:** Learn the concepts, terminology, and techniques in statistical prediction.\n\n## Acknowledgement\n\nThis lecture draws from Ryan Tibshirani's [Statistical Prediction lecture](http://www.stat.cmu.edu/~ryantibs/statcomp/lectures/prediction.html) for Statistical Computing at Carnegie Mellon University.\n\n# Training and Testing\n\n## Recall: Regression\n\nAssume we have some data:\n\n-   $X_1,\\ldots, X_p$: $p$ **independent variables/explanatory variables/covariates/predictors**\n-   $Y$: the **dependent variables/response/outcome**.\n\nWe want to know the relationship between our covariates and our response, we can do this with a method called **regression**. Regression provides us with a statistical method to conduct inference and prediction.\n\n. . .\n\nWhat is the difference between inference and prediction?\n\n## Recall: Regression\n\n-   **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance\n    -   What is the relationship between sleep and GPA?\n    -   Is parents' education or parents' income more important for explaining income?\n-   **prediction:** predict new/future outcomes from new/future covariates\n    -   Can we predict test scores based on hours spent studying?\n    \n## Linear Regression\n\nGiven our response $Y$ and our predictors $X_1, \\ldots, X_p$, a **linear regression model** takes the form:\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\nPreviously, we focused on using this model for inference (e.g. $H_0: \\beta_1 = 0$). Today, we are shifting our focus to prediction.\n\n## Statistical Prediction\n\nOften, we use a linear model even when we know that the assumptions for inference are broken: for example, when errors are not normally distributed.\n\nThough the model may be wrong, it may still be useful for learning about the relationship between response and covariates.\n\nIn addition, we can use a working model for **prediction**.\n\n## Training and testing data\n\nWhen training predictive algorithms, it is common to split your data into two categories:\n\n**Training data:** data used to train/fit our model\n\n**Test data:** data used to evaluate/test the performance of our model\n\nSuppose we have training data $X_{i1}, \\ \\ldots,\\ X_{ip}, Y_i, \\ i = 1,\\ldots,n$ which we use to estimate regression coefficents $\\hat\\beta_0, \\ \\hat\\beta_1, \\ldots \\hat\\beta_p$ (Recall: we use $\\hat{}$ \"hats\" to indicate **estimates**).\n\n## Making predictions\n\nNow suppose we are given new testing data $X^*_1,\\ \\ldots, X^*_{p}$ and asked to predict the associated $Y^âˆ—$. Using our estimated linear model, the prediction is: $$\\widehat{Y}^* = \\hat\\beta_0 + \\hat{\\beta}_1 X^*_1 + \\cdots + \\hat{\\beta}_p X^*_p$$\n\nGiven our prediction\n\n$$\\hat{Y} = \\hat\\beta_0 + \\hat{\\beta}_1 X^*_1 + \\cdots + \\hat{\\beta}_p X^*_p$$ We define the **mean squared error**, or **mean squared prediction error**, as\n\n$$\\mathbb{E}[(Y^* -\\hat{Y}^*)^2].$$\n\nIn other words, the test error is defined as the expected squared difference between a new prediction and the truth, where the expectation is taken over the population distribution of test data.\n\n## Test error\n\n**Mean squared prediction error:** $\\mathbb{E}[(Y^* -\\hat{Y}^*)^2],$ the expected squared difference between a new prediction and the truth.\n\nThe MSPE quantifies the predictive power of our model, but we do not actually have access to the full poulation distribution.\n\nAs such, we must estimate the MSPE using our data in order to use it to assess our model.\n\nIn particular, we could use the MSPE for:\n\n-   **predictive assessment:** understanding the magnitude of errors we should expect when making future predictions\n-   **model/method selection:** choosing among multiple candidate models.\n\n## Training Error\n\nA natural estimator for the test error might be the observed average **training error:**\n\n$$\\dfrac{1}{n}\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2.$$\n\nCan you think of a problem with this approach?\n\n. . .\n\nIn general, this will be *too optimistic*. Why? We obtained parameter estimates $\\widehat{\\beta}_0,\\ \\ldots,\\ \\widehat{\\beta}_p$ based on our training data.\n\n## Training Error\n\nMore specifically (with a linear regression model), we chose parameters to minimize the squared errors (hence, least squares).\n\nWe cannot expect this error rate to generalize to test data.\n\nCritically, the more complex our model, the more optimistic our training data will be as an estimate for our test data\n\n## Example: Training Error vs Test Error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggpubr)\n# generate data\nset.seed(302)\nn <- 30\nx <- sort(runif(n, -3, 3))\ny <- 2*x + 2*rnorm(n)\nx_test <- sort(runif(n, -3, 3))\ny_test <- 2*x_test + 2*rnorm(n)\ndf_train <- data.frame(\"x\" = x, \"y\" = y)\ndf_test <- data.frame(\"x\" = x_test, \"y\" = y_test)\n\n# store a theme\nmy_theme <- theme_bw(base_size = 16) + \n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5))\n\n# generate plots\ng_train <- ggplot(df_train, aes(x = x, y = y)) + geom_point() +\n  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + \n  labs(title = \"Training Data\") + my_theme\ng_test <- ggplot(df_test, aes(x = x, y = y)) + geom_point() +\n  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + \n  labs(title = \"Test Data\") + my_theme\nggarrange(g_train, g_test) # from ggpubr, to put side-by-side\n```\n:::\n\n\n## Example: Training Error vs Test Error\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Example: Training Error vs Test Error\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model and calculate training error\nlm_fit <- lm(y ~ x, data = df_train)\nyhat_train <- predict(lm_fit)\n# (y_i - yhat_i)^2\ntrain_err <- mean((df_train$y - yhat_train)^2)\n\n# Calculate testing error\nyhat_test <- predict(lm_fit, data.frame(x = df_test$x))\ntest_err <- mean((df_test$y - yhat_test)^2)\n\n# Add linear model and error text to plot\ng_train2 <- g_train + \n  labs(subtitle = paste(\"Training error:\", round(train_err, 3))) +\n  geom_line(aes(y = fitted(lm_fit)), col = \"red\", lwd = 1.5)\ng_test2 <- g_test + \n  labs(subtitle = paste(\"Test error:\", round(test_err, 3))) +\n  geom_line(aes(y = fitted(lm_fit), x = df_train$x), col = \"red\", lwd = 1.5)\n\nggarrange(g_train2, g_test2)\n```\n:::\n\n\n## Example: Training Error vs Test Error for linear model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Example: Degree 5 polynomial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 5 degree polynomial and calculate training error\nlm_fit_5 <- lm(y ~ poly(x, 5), data = df_train)\nyhat_train_5 <- predict(lm_fit_5)\ntrain_err_5 <- mean((df_train$y - yhat_train_5)^2)\n\n# Calculate testing error\nyhat_test_5 <- predict(lm_fit_5, data.frame(x = df_test$x))\ntest_err_5 <- mean((df_test$y - yhat_test_5)^2)\n\n# Create smooth line for plotting\nx_fit <- data.frame(x = seq(-3, 3, length = 100))\nline_fit <- data.frame(x = x_fit, y = predict(lm_fit_5, newdata = x_fit))\n                       \n# Add linear model and error text to plot\ng_train3 <- g_train + \n  labs(subtitle = paste(\"Training error:\", round(train_err_5, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\ng_test3 <- g_test + \n  labs(subtitle = paste(\"Test error:\", round(test_err_5, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\n\nggarrange(g_train3, g_test3)\n```\n:::\n\n\n## Example: Degree 5 polynomial\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Example: Degree 10 polynomial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 10 degree polynomial and calculate training error\nlm_fit_10 <- lm(y ~ poly(x, 10), data = df_train)\nyhat_train_10 <- predict(lm_fit_10)\ntrain_err_10 <- mean((df_train$y - yhat_train_10)^2)\n\n# Calculate testing error\nyhat_test_10 <- predict(lm_fit_10, data.frame(x = df_test$x))\ntest_err_10 <- mean((df_test$y - yhat_test_10)^2)\n\n# Create smooth line for plotting\nx_fit <- data.frame(x = seq(-3, 3, length = 100))\nline_fit <- data.frame(x = x_fit, y = predict(lm_fit_10, newdata = x_fit))\n                       \n# Add linear model and error text to plot\ng_train4 <- g_train + \n  labs(subtitle = paste(\"Training error:\", round(train_err_10, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\ng_test4 <- g_test + \n  labs(subtitle = paste(\"Test error:\", round(test_err_10, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\n\nggarrange(g_train4, g_test4)\n```\n:::\n\n\n## Example: Degree 10 polynomial\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Example: Degree 15 polynomial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 15 degree polynomial and calculate training error\nlm_fit_15 <- lm(y ~ poly(x, 15), data = df_train)\nyhat_train_15 <- predict(lm_fit_15)\ntrain_err_15 <- mean((df_train$y - yhat_train_15)^2)\n\n# Calculate testing error\nyhat_test_15 <- predict(lm_fit_15, data.frame(x = df_test$x))\ntest_err_15 <- mean((df_test$y - yhat_test_15)^2)\n\n# Create smooth line for plotting\nx_fit <- data.frame(x = seq(-3, 3, length = 100))\nline_fit <- data.frame(x = x_fit, y = predict(lm_fit_15, newdata = x_fit))\n                       \n# Add linear model and error text to plot\ng_train5 <- g_train + \n  labs(subtitle = paste(\"Training error:\", round(train_err_15, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\ng_test5 <- g_test + \n  labs(subtitle = paste(\"Test error:\", round(test_err_15, 3))) +\n  geom_line(data = line_fit, aes(y = y, x = x), col = \"red\", lwd = 1.5)\n\nggarrange(g_train5, g_test5)\n```\n:::\n\n\n## Example: Degree 15 polynomial\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Bias-Variance Tradeoff\n\nThe formula for test error can be decomposed as a function of the bias and variance of our estimates.\n\n-   **Bias:** the expected difference between our estimate and the truth\n\n-   **Variance:** the variability of our estimate of the truth\n\n## Bias-Variance Tradeoff\n\nFor example, suppose $Y^*=\\mu+\\epsilon$ where $\\epsilon$ is mean zero random error with variance $\\sigma^2$:\n\n$$\\begin{align}\\mathbb{E}[(Y^* -\\hat{Y}^*)^2]&=\\mathbb{E}(Y^{*2})-2\\mathbb{E}(Y^*\\hat{Y}^*)+\\mathbb{E}(\\hat{Y}^{*2})\\\\\n&=\\mu^2+\\sigma^2-2\\mu\\mathbb{E}(\\hat{Y}^*)+\\mathrm{Var}(\\hat{Y}^{*})+\\mathbb{E}(\\hat{Y}^{*2})\\\\\n&=(\\mu -\\mathbb{E}(\\hat{Y}^*))^2+\\sigma^2+\\mathrm{Var}(\\hat{Y}^{*}) \\end{align}$$\n\n## Bias-Variance Tradeoff\n\n<img src=\"images/bvtradeoff.png\" height=\"500\"/>\n\nImage Credit: An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\n\n## Bias-Variance Tradeoff\n\n-   **Bias:** underfitting, error from missing relevant relationships\n\n-   **Variance:** overfitting, error from high sensitivity\n\n<img src=\"images/underover1.png\" height=\"300\"/>\n\n[Image Source](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n\n## Bias-Variance Tradeoff\n\n<img src=\"images/underover2.png\" height=\"300\"/>\n\n[Image Source](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n\n## Bias-Variance Tradeoff\n\nIt is easy to fit a model with extremely low variance and extremely high bias, or extremely low bias and extremely high variance, but these models will not have a good test error. Ideally, we would like an estimate that has both low bias and low variance, but this is not always possible!\n\nAs a general rule of thumb, as we increase the complexity of our model, the bias will decrease but the variance will increase. You can think of our bias decreasing because we are training our model to be more specific to our training data. You can think of the variance increasing because our model will be overfit, and will vary substantially with new training data.\n\n# Cross-validation\n\n## Sample-splitting\n\nWhere do we get our \"training data\" and our \"test data\" in practice? We can't just simulate more data for our test data in the real world. In practice, we use a technique called **sample-splitting:**\n\n1.  Split the data set into two (or more...) parts\n2.  First part of data: train the model/method\n3.  Second part of data: make predictions\n4.  Evaluate observed test error\n\n## Sample-splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forcats)\n# Generate data\nset.seed(302)\nn <- 50\nx <- sort(runif(n, -3, 3))\ny <- 2*x + 2*rnorm(n)\n\n# Randomly split data into two parts\ninds <- sample(rep(1:2, length = n))\nsplit <- as.factor(inds) %>%\n  fct_recode(Training = \"1\", Test = \"2\")\ndata <- data.frame(\"x\" = x, \"y\" = y, \"split\" = split)\n\ng_split <- ggplot(data, aes(x = x, y = y, color = split)) + geom_point() +\n  labs(title = \"Sample-splitting\") + my_theme\ng_split\n```\n:::\n\n\n## Sample-splitting\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Sample-splitting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train on the training split\ndata_train <- data %>% filter(split == \"Training\")\nlm_1 <- lm(y ~ x, data = data_train)\nlm_10 <- lm(y ~ poly(x, 10), data = data_train)\n\n# Predict on the second half\ndata_test <- data |> dplyr::filter(split == \"Test\")\npred_1 <- predict(lm_1, data.frame(x = data_test$x))\npred_10 <- predict(lm_10, data.frame(x = data_test$x))\n\n# Calculate test error\ntest_err_1 <- mean((data_test$y - pred_1)^2)\ntest_err_10 <- mean((data_test$y - pred_10)^2)\n```\n:::\n\n\n## Sample-splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create smooth lines for plotting\nx_fit <- data.frame(x = seq(min(data$x), max(data$x), length = 100))\nline_fit_1 <- data.frame(x = x_fit, y = predict(lm_1, newdata = x_fit))\nline_fit_10 <- data.frame(x = x_fit, y = predict(lm_10, newdata = x_fit))\n                       \n# Add linear model and error text to plot\ng_split1 <- g_split + \n  labs(subtitle = paste(\"Degree 1 Test error:\", round(test_err_1, 3))) +\n  geom_line(data = line_fit_1, aes(y = y, x = x), col = \"black\", lwd = 1.5)\ng_split10 <- g_split + \n  labs(subtitle = paste(\"Degree 10 Test error:\", round(test_err_10, 3))) +\n  geom_line(data = line_fit_10, aes(y = y, x = x), col = \"black\", lwd = 1.5)\n\nggarrange(g_split1, g_split10, common.legend = TRUE, legend = \"bottom\")\n```\n:::\n\n\n## Sample-splitting\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Sample-splitting\n\nSample-splitting gives us a powerful tool to evaluate the performance of estimator *on data that was not used to train it*. Of course, this comes at the cost of sacrificing some of our data that could be used in order to train a model with as much information as possible, but this is a sacrifice we must make in practice if we want to appropriately evaluate the predictive performance of our models/methods!\n\nHowever, we can do better than just splitting our data in half.\n\n## k-fold Cross-validation\n\nIn general, the process of $k$-fold cross-validation is as follows:\n\n1.  Split data into $k$ parts (folds)\n2.  Use all but 1 fold as your training data and fit the model\n3.  Use the remaining fold for your test data and make predictions\n4.  Switch which fold is your test data and repeat steps 2 and 3 until all folds have been test data (k times)\n5.  Compute squared error\n\nCommonly, we use $k=5$ and $k=10$.\n\n## k-fold Cross-validation\n\n![](images/cv.png){fig-align=\"center\"}\n\n## k-fold Cross-validation\n\nFor each split, we calculate the out-of-sample mean squared prediction error. For example, for the first split, we calculate\n\n$$MSE_1 = \\dfrac{1}{m}\\sum_{j = 1}^{m} (Y_{j,test} - \\hat{Y}_{j,test})^2,$$ where $m$ is the number of observations in the test set.\n\nThen we calculate the **cross-validation estimate** of our test error\n\n$$CV_{(k)} = \\dfrac{1}{k}\\sum_{i=1}^{k}MSE_i.$$\n\nIn words, our cross-validation estimate of the test error is the average of the mean squared errors across the splits!\n\n## Leave-One-Out Cross-Validation\n\nLeave-one-out cross-validation (LOOCV) is a special instance of $k$-fold cross-validation where $k$ is equal to the sample size $n$. This means you fit $n$ different models using $n-1$ data points for your training data, and then test on the remaining individual data point.\n\n## Bias-variance Tradeoff\n\nLOOCV strengths in that it uses as much data as possible to train the model while still having out-of-sample test data. Thus, it will provide approximately unbiased estimates of the test error, and is the best choice for minimizing bias.\n\nHowever, LOOCV has one major drawback: variance. Because it fits $n$ models, each of which is trained on almost identical training data, the outputs of the models are highly correlated. All other things being equal, the mean of correlated observations has a higher variance than the mean of uncorrelated observations. Thus, LOOCV has higher variance than $k$-fold CV with $k<n$.\n\nAs a rule of thumb, stick to $k=5$ and $k=10$. These values tend to result in an ideal balance in terms of the bias-variance tradeoff.\n\n## What to do after cross-validation?\n\nWe use cross-validation as a method to evaluate our model, *not* to build our model! Thus, once we have used $k$-fold cross-validation to come up with a reasonable estimate of our out-of-sample test-error (and perhaps to choose among several models based on this test error), we use the *full* data to train our final predictive model.\n\nThus, using cross-validation, we are able to build a model based on our full data, and still evaluate the performance of our model on out-of-sample data!\n\n## k-fold Cross-validation Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(302)\n# Split data in 5 parts, randomly\nk <- 5\ninds <- sample(rep(1:k, length = n))\n# Use data from before\ndata <- data.frame(\"x\" = x, \"y\" = y, \"split\" = inds)\n\n# Empty matrix to store predictions, 2 columns for 2 models\npred_mat <- matrix(NA, n, 2)\nfor (i in 1:k) {\n  data_train <- data |> dplyr::filter(split != i)\n  data_test <- data |> dplyr::filter(split == i)\n  \n  # Train our models\n  lm_1_cv <- lm(y ~ x, data = data_train)\n  lm_10_cv <- lm(y ~ poly(x, 10), data = data_train)\n  \n  # Record predictions\n  pred_mat[inds == i, 1] <- predict(lm_1_cv, data.frame(x = data_test$x))\n  pred_mat[inds == i, 2] <- predict(lm_10_cv, data.frame(x = data_test$x))\n}\n\n# Compute average MSE to get CV error\ncv_err <- colMeans((pred_mat - data$y)^2)\n```\n:::\n\n\n\n\n## k-fold Cross-validation Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictive model\nlm_1 <- lm(y ~ x, data = data)\nlm_10 <- lm(y ~ poly(x, 10), data = data)\n\n# Create smooth lines for plotting\nx_fit <- data.frame(x = seq(min(data$x), max(data$x), length = 100))\nline_fit_1 <- data.frame(x = x_fit, y = predict(lm_1, newdata = x_fit))\nline_fit_10 <- data.frame(x = x_fit, y = predict(lm_10, newdata = x_fit))\n\n# Recode split for legend\ndata <- data %>% \n  mutate(Fold = fct_recode(as.factor(split), \"Fold 1\" = \"1\", \"Fold 2\" = \"2\",\n                           \"Fold 3\" = \"3\", \"Fold 4\" = \"4\", \"Fold 5\" = \"5\"))\n\ng_cv <- ggplot(data, aes(x = x, y = y, color = Fold)) + geom_point() +\n  labs(title = \"Cross-validation\") + my_theme\ng_cv1 <- g_cv + \n  labs(subtitle = paste(\"Degree 1 CV error:\", round(cv_err[1], 3))) +\n  geom_line(data = line_fit_1, aes(y = y, x = x), col = \"black\", lwd = 1.5)\ng_cv10 <- g_cv + \n  labs(subtitle = paste(\"Degree 10 CV error:\", round(cv_err[2], 3))) +\n  geom_line(data = line_fit_10, aes(y = y, x = x), col = \"black\", lwd = 1.5)\nggarrange(g_cv1, g_cv10, common.legend = TRUE, legend = \"bottom\")\n```\n:::\n\n\n## k-fold Cross-validation Example\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26_prediction_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Statistical Prediction Algorithms\n\nWe've talked about how regression can be used for both statistical inference and prediction. We are now going to shift to talk about algorithms designed specifically for statistical prediction.\n\nWe will discuss only a few methods for this course to teach you about the principles of how they work. This will serve as an introduction to topics you may encounter if you venture further into fields such as **machine learning**.\n\n## Statistical Prediction Algorithms\n\nThe normal distribution, specified by parameters for the mean and the variance, is an example of a **parametric model**. We are now going to discuss some **non-parametric models**. As the name implies, these models do not assume that the data are drawn from some distribution with a fixed number of parameters.\n\nNon-parametric models are not motivated by inference on model parameters. Instead, the focus is often on pure prediction. How can we use data to help us classify and predict the world around us? For the most part, many of these methods would have been impossible before the rise of high-powered computing.\n\n## k-nearest neighbors\n\nk-nearest neighbors is a non-parametric prediction algorithm.\n\n-   **Input:** training data $X_i = (X_{i1}, \\ldots, X_{ip})$ and output class $Y_i$ for $i = 1,\\ldots, n$\n-   **Input:** test data $X^* = (X^*_1, \\ldots, X^*_p)$ with unknown output class $Y^*$\n-   **Algorithm:** Find $X_{(1)},\\ldots, X_{(k)}$, the $k$ nearest points to $X^*$\n-   **Output:** Predicted class $\\hat{Y}^*$, the most common class from $X_{(1)},\\ldots, X_{(k)}$\n\n\n\n## k-nearest neighbors\n\nThis method is simple and highly flexible, but it can also be slow and inappropriate for certain settings.\n\n![](images/knn.png){fig-align=\"center\"}\n\nImage Credit: An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani \n\n\n## k-nearest neighbors\n\n\n![](images/knn2.png){fig-align=\"center\"}\n\nImage Credit: An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\n\n## Choosing k\n\nWhat happens when $k=1$? $k = n$?\n\nIn order to choose an \"optimal\" $k$, it is common to use cross-validation:\n\n-   For $k=1,\\ldots,n$ (or a more restrictive range), fit $k$-nearest neighbors\n-   Use cross-validation to identify $k$ with the minimum test error\n-   Re-train a $k$-nearest neighbor model using all the data with optimal choice of $k$\n\nCross-validation is a great tool to choose values for tuning parameters across a number of algorithms, not just $k$-nearest neighbors!\n\n## Choosing k\n\n![](images/choosek.png){fig-align=\"center\"}\n\n## Choosing k\n\n![](images/doubledescent.png){fig-align=\"center\"}\n\n\n\n![Image Credit](https://en.wikipedia.org/wiki/Double_descent#/media/File:Double_descent_in_a_two-layer_neural_network_(Figure_3a_from_Rocks_et_al._2022).png)\n\n\n## Trees\n\nInstead of classifying based on neighboring data points in the training data, we can classify based on splits in our covariates.\n\n-   **Input:** training data $X_i = (X_{i1}, \\ldots, X_{ip})$ and output class/value $Y_i$ for $i = 1,\\ldots, n$\n-   **Input:** test data $X^* = (X^*_1, \\ldots, X^*_p)$ with unknown output class/value $Y^*$\n-   **Algorithm:** Identify splits in $X_i$ that best predict $Y_i$\n-   **Output:** Predicted class/value $\\hat{Y}^*$ using splits\n\n## Trees\n\n\nTrees to predict class are referred to as **classification trees**. Trees to predict quantitative output are referred to as **regression trees**.\n\nThis algorithm is easy, efficient, and interpretable, but it is not as flexible as $k$-nearest neighbors.\n\n![](images/tree.png){fig-align=\"center\"}\n\n[Image source](http://www.stat.cmu.edu/~ryantibs/statcomp/lectures/prediction.html) \n\n\n\n## Random Forest\n\nIn practice, we don't typically predict with a single tree. Instead, we combine multiple trees and take the average/most common of their predictions. Keeping with the tree metaphor, this is referred to as the **random forest** algorithm.\n\n## Random Forest\n\n![](images/randomforest.png){fig-align=\"center\"}\n\n[Image source](https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249)\n\n## Random Forest\n\n![](images/randomforest2.jpeg){fig-align=\"center\"}\n\n[Image source](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n\n## Variable Importance\n\nOne major benefit of tree-based prediction algorithms is that they provide a powerful and interpretable tool for assessing **variable importance**. For example, we can look across all trees in a random forest, assess how often variables were used for splitting and how helpful those splits were for prediction, and identify the variables that improve our predictions the most!\n\n(For more on this, see the *Gini index* and \"An Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.)\n\n## Statistical Prediction Algorithms\n\nThis was a very brief overfiew of a very small subset of the algorithms used for statistical prediction. Other common examples include:\n\n-   neural networks,\n-   support vector machines,\n-   kernel classifiers,\n-   Bayesian classifiers, \n-   and more... \n\nThese algorithms, and related topics, are likely things that you will cover if you take courses more focused on machine learning.\n",
    "supporting": [
      "26_prediction_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}