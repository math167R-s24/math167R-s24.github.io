{
  "hash": "19e3397ddd720322ec71097f011b1bf0",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Hypothesis testing\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n## Overview of today\n\n-   Review of hypothesis testing\n-   Type I and II error\n-   Hypothesis tests in R\n-   Reproducibility\n\n## Motivation\n\nHypothesis testing always begins with a scientific question of interest. Suppose we have a coin, and we want to test whether or not it is fair. We might flip the coin over and over and record the results, and then measure the proportion of times we got a heads.\n\nWe can state this experiment more formally. Let $X_1,...,X_n$ be $n$ random variables corresponding to an indicator of whether a coin flip is heads (i.e. $X_i = 1$ if the $i$<sup>th</sup> flip is heads, $0$ if tails). We don't know the *true* probability that our coin lands on heads, so we may say that $$X_i \\stackrel{iid}{\\sim} \\mathrm{Binomial}(1, p),$$ with unknown probability parameter $p$.\n\n## Motivation\n\nWe are interested in whether our coin is fair, which corresponds to $p=1/2$. This hypothesis we are interested in testing will be our **null hypothesis**, denoted $H_0$ (pronounced \"H-naught\" or \"H-zero\").\n\nAlternatively, we may think that our coin is biased in either direction (i.e. either lands on heads too often or lands on tails too often). This corresponds to $p \\neq 1/2$. This is our **alternative hypothesis**, denoted $H_a$ (or sometimes $H_1$).\n\nThus, our hypothesis test is as follows:\n\n\n```{=tex}\n\\begin{align}\nH_0: p&=1/2\\\\\nH_a: p&\\neq 1/2\n\\end{align}\n```\n\n## The hypotheses\n\nIn general, we can think of\n\n-   **Null hypothesis** $H_0$: our default assumption to be tested, our baseline, no relationships/differences/values of our parameters of interest. *Assumed to be true unless evidence suggests otherwise!*\n\n-   **Alternative hypothesis** $H_a$: the alternative to our null, something interesting is happening in our data, our scientific hypothesis of interest\n\nAn important note: for the most part, these are broad generalizations, not formal definitions.\n\nIt is not always the case that the alternative is \"interesting\" and the null is not.\n\n## Hypothesis testing\n\n**We always assume that our null hypothesis is true to compute the null distribution.**\n\nOur question is whether our data provides sufficient evidence to **reject the null** $H_0$.\n\nFor example, if we flipped our coin 100 times and observed 99 heads, we probably would reject our null hypothesis that the coin is fair. If instead we flipped our coin 100 times and observed 52 heads, we probably would not conclude that we have enough evidence to reject our null hypothesis, and maintain our baseline assumption that the coin is fair.\n\n## Hypothesis testing\n\nWhere we assume something is true, and then observe whether or not the evidence (data) supports this assumption.\n\nIn hypothesis testing, we never, ever, ever examine whether our evidence supports our scientific hypothesis (the alternative). We only examine whether it supports rejecting a baseline assumption (the null).\n\n## Statistical inference\n\n-   **Statistical inference:** using information from a sample to draw inferences about a population.\n\n-   Usually we are interested in studying/estimating a population **parameter**: a numerical characteristic of our population/model.\n\n-   Example of a model/population: an infinite sequence of coin flips distributed $Bin(1, p)$.\n\n-   Example of a parameter: the probability of heads $p$.\n\n## Estimates and Estimators\n\nWe use **estimators** to calculate **estimates** for our **parameters.**\n\n-   **Estimator:** a function of our data (for example, the sample mean)\n\n-   **Estimate:** the actually observed value of our estimator, anumber calculated from our data (for example, an observed sample mean of 52).\n\n## Example\n\nBack to our example, recall that we aim to test\n\n\n```{=tex}\n\\begin{align}\nH_0: p&=1/2\\\\\nH_a: p&\\neq 1/2\n\\end{align}\n```\n\nusing a random variable representing a heads $$X_i \\stackrel{iid}{\\sim} Bin(1, p).$$ Let's assume that we have the patience to flip our coin $n=100$ times.\n\n## Example: flipping coins\n\nInstead of actually flipping the coin, let's carry out this example in R.\n\n\n::: {.cell hash='22_hypothesis-testing_cache/revealjs/unnamed-chunk-1_2121ce22ce4e1ccb445d1ce94de866a4'}\n\n```{.r .cell-code}\nset.seed(302)\n# the true population parameter (in practice, we don't know this).\np <- 0.6\n# simulate 100 coin flips\ncoin_flips <- rbinom(100, size = 1, prob = p)\nhead(coin_flips)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 0 0 1 1 1\n```\n:::\n\n```{.r .cell-code}\nsum(coin_flips)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 61\n```\n:::\n:::\n\n\nWe observed 61 heads out of 100 coin flips. This is our **test statistic**, a statistic calculated from our sample data which we will use to evaluate our hypotheses.\n\n## Example: flipping coins\n\nWe observed 61 heads out of 100 coin flips.\n\nIs this unusual enough to reject $H_0$?\n\nIn order to answer this, we must ask ourselves: if we assume $H_0$ is true, how unlikely is what we observed? *Before* we conduct our experiment, we must decide what cut-off we would like to use to determine whether or not to reject $H_0$. This cut-off is typically denoted by the greek letter $\\alpha$.\n\nTypically, we reject $H_0$ if the probability of observing a test statistic *as or more extreme* than what we observed is less than 5%. We will use this cut-off, even though it is arbitrary.\n\nThus, we set $\\alpha = 0.05$.\n\n## Example: flipping coins\n\nWe can use `pbinom()` to calculate the probability of observing a test statistic as extreme or more extreme than 61.\n\n-   *\"as extreme or more extreme\"*: Our alternative $H_a: p \\neq 1/2$ means that we are interested in deviations in *either* direction. Thus, we must consider not only the probability of observing *more than* 61 heads, but also the probability of observing *less than* 39 heads, because this is just as extreme in the other direction.\n\n## Example: flipping coins\n\nNote that `lower.tail = TRUE` is the default, and returns the probability of observing a value *less than or equal to* the value that we input. Thus, when `lower.tail = FALSE`, we return the probability of observing a value *strictly greater than* the value that we input. We want the probability of observing a value *greater than or equal to* the value that we input. Thus, we subtract 1 to make sure this value is included in our probability.\n\n## Example: flipping coins\n\n\n::: {.cell hash='22_hypothesis-testing_cache/revealjs/unnamed-chunk-2_8bfdb7b30494963c2f948dcd39c8a612'}\n\n```{.r .cell-code}\n# record test statistic\ntest_stat <- sum(coin_flips)\n# record how \"extreme\" our test statistic is\ndiff_from_exp <- abs(50 - test_stat)\n# first component: lower tail\n# second component: upper tail\np_val <- pbinom(50 - diff_from_exp, size = 100, prob = 0.5) +\n  pbinom(50 + diff_from_exp - 1, size = 100, prob = 0.5, lower.tail = FALSE) \nround(p_val, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.035\n```\n:::\n:::\n\n\n## Example: flipping coins\n\nNote that `lower.tail = TRUE` is the default, and returns the probability of observing a value *less than or equal to* the value that we input. Thus, when `lower.tail = FALSE`, we return the probability of observing a value *strictly greater than* the value that we input. We want the probability of observing a value *greater than or equal to* the value that we input. Thus, we subtract 1 to make sure this value is included in our probability.\n\n$P(\\text{test statistic equally or more extreme than observed}|H_0) =$ 0.035.\n\n## Example: flipping coins\n\n$P(\\text{test statistic equally or more extreme than observed}|H_0) =$ 0.035.\n\nThis is less than our pre-determined cut-off of 0.05, so we conclude that our results are **statistically significant**. This means that we reject $H_0$ and have evidence to support our alternative hypothesis $H_a$, that our coin is biased. With that conclusion, we have completed our hypothesis test.\n\nNote that we do **not** conclude that $H_0$ is false, or that $H_a$ is true. We only state our conclusion in terms of sufficient evidence to reject the null or insufficient evidence to reject the null.\n\n## Interpreting $P$-values\n\nRemember, a $P$-value is not the probability our null hypothesis is true. A $P$-value only describes the probability of observing results as extreme or more extreme than our results **under the null distribution**.\n\nProblems begin when our null distribution is misspecified (for example, when assumptions are broken).\n\n## Visualizing $P$-values\n\n![(Devore)](images/Screenshot%202023-10-28%20at%2010.30.53%20AM.png)\n\n## Summary: hypothesis testing\n\n1.  Define null and alternative hypotheses.\n2.  Compute a test statistic for which the null distribution is known.\n3.  Compare the test statistic with the null distribution to obtain a P-value.\n\n## Example: two sample $t$-test\n\n1.  Hypotheses:\n\n\n```{=tex}\n\\begin{align}\nH_0: \\mu_x-\\mu_y &= 0\\\\\nH_a: \\mu_x-\\mu_y&\\neq 0\n\\end{align}\n```\n\n2.  Compute sample means $\\overline{X}$ and $\\overline{Y}$ and the test statistic $T=\\frac{\\overline{X}-\\overline{Y}}{s_p\\sqrt{1/n_x+1/n_y}}$ (assuming equal variances). Here, $s_p$ is the pooled variance estimate.\n3.  Compare $T$ with a t-distribution with $n_1+n_2-2$ degrees of freedom.\n\n## `t.test()`: t-tests in R\n\nConducting t-tests in R is very easy using the function `t.test()`. For a **two sample t-test** to compare the means of two samples, just provide the two samples in R. Here I use randomly simulated data. This performs a test of the form:\n\n$$\n\\begin{align}\nH_0: \\mu_x &= \\mu_y\\\\\nH_a: \\mu_x &\\neq  \\mu_y \n\\end{align}\n$$\n\n## `t.test()`: t-tests in R\n\n\n::: {.cell hash='22_hypothesis-testing_cache/revealjs/unnamed-chunk-3_89ad5adfa100a95dfcbb96f06d73a288'}\n\n```{.r .cell-code}\nset.seed(302)\nx <- rnorm(10, mean = 0, sd = 1)\ny <- rnorm(10, mean = 1.5, sd = 1)\nt.test(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  x and y\nt = -1.5143, df = 16.95, p-value = 0.1484\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.627018  0.267543\nsample estimates:\nmean of x mean of y \n0.3492983 1.0290359 \n```\n:::\n:::\n\n\n## `t.test()`: t-tests in R\n\nNote that we can also test **one-sided** hypotheses:\n\n$$\n\\begin{align}\nH_0: \\mu_x &= \\mu_y\\\\\nH_a: \\mu_x &< \\mu_y \n\\end{align}\n$$\n\n\n::: {.cell hash='22_hypothesis-testing_cache/revealjs/unnamed-chunk-4_27c3aa6090d3757bf4c57cb56a4bd819'}\n\n```{.r .cell-code}\nt.test(x, y, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  x and y\nt = -1.5143, df = 16.95, p-value = 0.07419\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf 0.1012809\nsample estimates:\nmean of x mean of y \n0.3492983 1.0290359 \n```\n:::\n:::\n\n\n## `t.test()`: t-tests in R\n\nWe can also use a **one sample t-test** to compare the means of one sample to some hypothesized value.\n\nThis performs a test of the form:\n\n$$\n\\begin{align}\nH_0: \\mu_x &= 0\\\\\nH_a: \\mu_x &\\neq 0\n\\end{align}\n$$\n\n## `t.test()`: t-tests in R\n\n\n::: {.cell hash='22_hypothesis-testing_cache/revealjs/unnamed-chunk-5_fbb1595af6bd2677e463a377f4f7bb70'}\n\n```{.r .cell-code}\nt.test(x, mu = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  x\nt = 0.98472, df = 9, p-value = 0.3505\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.4531331  1.1517298\nsample estimates:\nmean of x \n0.3492983 \n```\n:::\n:::\n\n\n## Type I and II errors\n\n-   A **type I error** consists of rejecting the null hypothesis when it is true.\n-   A **type II error** involves not rejecting the null hypothesis when it is false.\n\n. . .\n\nWhich is worse?\n\n. . .\n\nIt probably depends on the scenario.\n\n## Type I errors\n\nType I errors are false positives--we obtain a \"statistically significant\" result via chance.\n\nNote that we typically control the type I error through the significance level $\\alpha$.\n\n$$\\Large \\alpha = P(\\text{reject } H_0 | H_0 \\textrm{ true}))$$\n\n$$\\Large 1-\\alpha = P(\\text{don't reject } H_0 | H_0 \\textrm{ true})$$\n\n## Type II errors\n\n**Type 2 errors** are false negatives--we incorrectly fail to reject the null hypothesis when the null hypothesis is actually false.\n\n$$ P(\\text{type 2 error}) =  P(\\text{fail to reject } H_0 | H_0 \\text{ is false})$$\n\nType 2 error also gives us the concept of **power**, the probability of correctly rejecting the null.\n\n$$ \\text{Power} = P(\\text{reject } H_0 | H_0 \\text{ is false})$$\n\n## Type I and II errors\n\n-   We can decrease the probability of a Type II errors by increasing our Type I error.\n-   We can decrease the probability of a Type II error *without sacrificing* Type I error by increasing our sample size.\n\n## Reproducibility and P-hacking\n\nIn many scientific settings, there has been an emphasis on producing results with statistically significant p-values. However, note that by repeatedly performing hypothesis tests, you are very likely to produce statistically significant results, even if the null hypothesis is true.\n\nThis has led to a **crisis in replication**, in which the results of many scientific studies are difficult or impossible to replicate.\n\n## Reproducibility and P-hacking\n\n**P-hacking** refers to the practice of repeatedly performing hypothesis tests (and potentially manipulating the data) until a statistically significant P-values is obtained. Usually, only this final result is published, without mentioning all of the manipulations that came before.\n\nFor this reason, it's crucial to document carefully everything you do in the data analysis pipeline to ensure that your work is taken seriously. Publishing code and data can help others to reproduce your research.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}