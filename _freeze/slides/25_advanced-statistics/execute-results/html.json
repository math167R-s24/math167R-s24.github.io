{
  "hash": "e629d3106ddd06fc70ad1b2381a12fea",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Advanced statistical methods\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n## Overview of today\n\n-   ANOVA\n-   The $F$-test for Regression Analysis\n-   Logistic regression\n\n## Analysis of Variance\n\nThe analysis of variance or ANOVA is a collection of statistical methods used to compare two or more groups to assess the differences between group means.\n\nThe simplest ANOVA provides a test of whether two or more population means are equivalent. As such the ANOVA is a generalization of the two sample $t$-test.\n\nANOVA is typically said to be introduced by Ronald Fisher, who applied the method to study the variation in crop yield related to different fertiliser treatments.\n\n## One-way ANOVA\n\nOne-way ANOVA or single-factor ANOVA is used to compare two or more population means. Let $I$ be the number of groups/populations and $\\mu_I$ to be the population mean of the response of interest for group $I$.\n\n**Hypotheses**:\n\n-   $H_0:\\mu_1=\\ldots=\\mu_I$\n-   $H_a: \\text{at least two of the means are different}$\n\n**Assumptions**:\n\n-   The population distributions are all normal with the same variance $\\sigma^2$.\n\n## Test Statistic\n\nThe $F$ test is commonly used to compare the causes of the total variation. In particular, the $F$ test statistic is defined as\n\n$$F=\\frac{\\text{variance between groups}}{\\text{variance within groups}}$$\n\nIf $F$ is large, then there may be evidence to reject the hypothesis that the group means are all equal.\n\n## Test Statistic\n\n$$F=\\frac{\\text{variance between groups}}{\\text{variance within groups}}=\\frac{\n\\text{Mean Square for Treatments}}{\\text{Mean Square for Error}}$$\n\nIn a balanced design, where each group has the same number of $J$ samples, and $X_{i, j}$ denotes the $j$th measurement from group $i$, the mean square terms have the following formulas:\n\n$$\\text{Mean Square for Treatments}=\\frac{J}{I-1}\\sum_i(\\overline{X}_i-\\overline{X}_{\\cdot\\cdot})^2$$\n\nwhere $\\overline{X}_i$ is the sample group mean and $\\overline{X}_{\\cdot\\cdot}$ is the overall sample mean.\n\n$$\\text{Mean Square for Error}=\\frac{S_1^2+S_2^2+\\cdots+S_I^2}{I}$$\n\n## Example: `PlantGrowth`\n\nThe `PlantGrowth` data provides results from an experiment comparing plant growth under a control and two different treatment conditions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(weight ~ group, data = PlantGrowth, main = \"PlantGrowth data\",\n        ylab = \"Dried weight of plants\", col = \"lightgray\")\n```\n\n::: {.cell-output-display}\n![](25_advanced-statistics_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Example: `PlantGrowth`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_res <- anova(lm(weight ~ group, data = PlantGrowth))\nanova_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Obtaining a $p$-value with an $F$-test statistic\n\nWhen $H_0$ is true, $F$ follows an $F$-distribution with $\\nu_1=I-1$ and $\\nu_2=I(J-1)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nx <- data.frame(x = seq(0, 8, length.out = 3))\nggplot(x, aes(x = x)) + \n  geom_function(fun = df, args = list(df1 = 2, df2 = 27)) + \n  geom_vline(xintercept = anova_res$`F value`, color = \"red\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](25_advanced-statistics_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## The $F$-distribution\n\nThe $F$ distribution has two parameters:\n\n-   numerator degrees of freedom $\\nu_1$\n-   denominator degrees of freedom $\\nu_2$\n\nThe $F$ distribution was derived as the ratio of (scaled) chi-square random variables and is frequently used to model the null distribution of test statistics.\n\n$$F = \\frac{U_1/d_1}{U_2/d_2}$$\n\nwhere $U_1$ is chi-square with $d_1$ degrees of freedom, $U_2$ is chi-square with $d_2$ degrees of freedom, and $U_1$ and $U_2$ are independent.\n\n## Example: `emails`\n\nThe `emails` data provides information on emails categorized as spam and non-spam.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemails <- read.csv(\"https://www.openintro.org/data/csv/email50.csv\")\nboxplot(num_char ~ spam, data = emails, main = \"Emails data\",\n        ylab = \"Length of email (characters)\", col = \"lightgray\")\n```\n\n::: {.cell-output-display}\n![](25_advanced-statistics_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Example: `emails`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_res <- anova(lm(num_char ~ spam, data = emails))\nanova_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: num_char\n          Df Sum Sq Mean Sq F value Pr(>F)\nspam       1    0.1   0.052   3e-04 0.9863\nResiduals 48 8441.3 175.860               \n```\n:::\n:::\n\n\n## The General Linear $F$-test\n\nIn general, the `anova()` function can be used to test the fit of a model for sample data.\n\nFor a simple linear regression case, the $F$ test can be used to compare a **full** model of the form\n\n$$y_i = (\\beta_0+\\beta_1x_{i1})+\\epsilon_i$$\n\nwith a **reduced** model:\n\n$$y_i=\\beta_0+\\epsilon_i$$\n\n**Hypotheses**:\n\n-   $H_0:y_i=\\beta_0+\\epsilon_i$\n-   $H_a: y_i=\\beta_0+\\beta_1x_{i1}+\\epsilon_i$\n\n## The General Linear $F$-test\n\nLet $p_{full}$ be the number of parameters in the full model and $p_{reduced}$ be the number of parameters in the reduced model.\n\nLet $SSE_{full}$ and $SSE_{reduced}$ be the sum of squared errors for the full and reduced models, respectively.\n\n$$\nF = \\frac{(SSE_{reduced}-SSE_{full})/(p_{full}-p_{reduced})}{SSE_{full}/p_{full}}\n$$\n\nThen $F$ follows an $F$ distribution with $p_{full}-p_{reduced}$ and $p_{full}$ degrees of freedom.\n\n## Example: `mtcars`\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-6_fc0669dbc59445fad52164a0f7ffa8ff'}\n\n```{.r .cell-code}\nmy_lm <- lm(mpg ~ wt, data = mtcars)\nanova(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nwt         1 847.73  847.73  91.375 1.294e-10 ***\nResiduals 30 278.32    9.28                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Example: `mtcars`\n\nNote that this is equivalent to the Wald test for the coefficient for `wt` in the univariate case:\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-7_019af16f285e2d0263baeaa0d958d784'}\n\n```{.r .cell-code}\nsummary(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,\tAdjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n```\n:::\n:::\n\n\n## The General Linear $F$-test\n\nMore generally the $F$-test can be used to test sets of nested models, when a reduced model can be compared with a full model.\n\n**Hypotheses**:\n\n-   $H_0: \\text{reduced model true}$\n-   $H_a: \\text{full model fits better}$\n\n## Example: `mtcars`\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-8_ea9eb877deeb2a4594bf659f6453bf19'}\n\n```{.r .cell-code}\nmy_lm <- lm(mpg ~ hp + wt, data = mtcars)\nanova(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nhp         1 678.37  678.37 100.862 5.987e-11 ***\nwt         1 252.63  252.63  37.561 1.120e-06 ***\nResiduals 29 195.05    6.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWhen we have more than one explanatory variable, the $F$-test is different from the univariate Wald test.\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-9_e6fbe068472e5e14be868d6c74a07947'}\n\n```{.r .cell-code}\nsummary(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n:::\n:::\n\n\n## Example: `mtcars`\n\nWe can explicitly specify the full and reduced models:\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-10_63e21054965ef28f44e6220c82d0cc88'}\n\n```{.r .cell-code}\nmy_lm_full <- lm(mpg ~ hp + wt, data = mtcars)\nmy_lm_reduced <- lm(mpg ~ 1, data = mtcars)\nanova(my_lm_full, my_lm_reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ 1\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n1     29  195.05                                  \n2     31 1126.05 -2      -931 69.211 9.109e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Logistic regression\n\nWhen we have binary outcomes $Y_1,\\ldots, Y_n$ where $Y_i\\in \\{0, 1\\}$ for all $i$, it may be inappropriate to use linear regression to model the relationships between $Y$ and covariates $X$.\n\n**Logistic regression** can be used to relate the **probability** that $Y_i=1$ to available covariates.\n\n## Logistic regression\n\n**Model:**\n\n$$P(Y_i=1)=p_i$$ $$\\text{logit}(p_i)=\\log\\left(\\frac{p_i}{1-p_i}\\right)=\\mathbf{x}_i^\\top\\boldsymbol\\beta$$ where $\\mathbf{X}_i$ is the covariate vector for individual $i$ and $\\boldsymbol\\beta$ is a vector of regression covariates.\n\nNote that the logit transformation has domain $(0,1)$ and range $(-\\infty, \\infty)$.\n\n## Logistic regression\n\nWhen we have only one covariate $x$, the model is as follows:\n\n$$\\text{logit}(p(x))=\\log\\left(\\frac{p(x)}{1-p(x)}\\right)=\\beta_0+\\beta_1 x$$\n\nwhere $p(x)$ represents the probability that an individual with covariate value $x$ experiences the response/event of interest.\n\n## Example: *Challenger* disaster\n\nOn January 28, 1986, the Challenger space shuttle suffered an accident shortly after launch, killing all crew members on board. An investigation suspected that damage to a set of critical parts called O-rings caused the disaster. It is believed that the damage is related to the temperature at launch time. The following data summarize data on O-rings for 23 shuttle missions.\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-11_f3685b9405425053a132b8883a51e5da'}\n\n```{.r .cell-code}\norings <- readr::read_csv(\"https://www.openintro.org/data/csv/orings.csv\")\nhead(orings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 4\n  mission temperature damaged undamaged\n    <dbl>       <dbl>   <dbl>     <dbl>\n1       1          53       5         1\n2       2          57       1         5\n3       3          58       1         5\n4       4          63       1         5\n5       5          66       0         6\n6       6          67       0         6\n```\n:::\n:::\n\n\n## Example: *Challenger* disaster\n\nThis plot visualizes the empirical proportion of damaged O-rings for each mission:\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-12_ece7a339b6c338d1168a7e5965cd54ea'}\n\n```{.r .cell-code}\nplot((damaged / 6) ~ temperature, data = orings, pch = 16,\n     ylab = \"Proportion of O-rings damaged\", xlab = \"Temperature (F)\")\n```\n\n::: {.cell-output-display}\n![](25_advanced-statistics_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Example: *Challenger* disaster\n\nThe `glm()` function can be used to fit generalized linear models including logistic regression models. The `family` argument is used to specify the distribution of the response variable (in this case, binomial for a logistic rgression).\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-13_ff3cab426708c3dfceaa9095d2492b36'}\n\n```{.r .cell-code}\norings_res <- glm((damaged > 0) ~ temperature, data = orings, family = binomial)\nsummary(orings_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = (damaged > 0) ~ temperature, family = binomial, \n    data = orings)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  15.0429     7.3786   2.039   0.0415 *\ntemperature  -0.2322     0.1082  -2.145   0.0320 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 28.267  on 22  degrees of freedom\nResidual deviance: 20.315  on 21  degrees of freedom\nAIC: 24.315\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\n## Example: *Challenger* disaster\n\nHere, our estimate of $\\beta_0$ is $b_0=15.04$ and our estimate of $\\beta_1$ is $b_1=-.23$.\n\nWe can interpret $\\beta_1$ as follows: for each additional degree of ambient temperature, the log-odds of experiencing an O-ring failure decreases by $.23$.\n\nIn other words, the odds changes by a factor of $\\exp(.23)\\approx.79$ as temperature increases by one degree.\n\nNote that the $p$-value for $\\beta_1$ is significant at the $.05$ level, suggesting evidence that temperature is associated with O-ring failure.\n\n## Example: *Challenger* disaster\n\nWe can use many of the functions we used with `lm` objects, including:\n\n-   `fitted()` returns the predicted probabilities for a logistic regression\n-   `predict()` can be used to generate predictions\n-   `plot()` returns a series of diagnostic plots\n-   `residuals()` returns residuals--not that these are not usually the typical observed minus predicted residuals.\n-   `anova()` can be used to assess the model fit for `glm` objects.\n\n## Example: *Challenger* disaster\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-14_8766ad5d13e7dc1e2997c651922ce6f6'}\n\n```{.r .cell-code}\nfitted(orings_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6          7 \n0.93924781 0.85931657 0.82884484 0.60268105 0.43049313 0.37472428 0.37472428 \n         8          9         10         11         12         13         14 \n0.37472428 0.32209405 0.27362105 0.22996826 0.22996826 0.22996826 0.22996826 \n        15         16         17         18         19         20         21 \n0.15804910 0.12954602 0.08554356 0.08554356 0.06904407 0.06904407 0.04454055 \n        22         23 \n0.03564141 0.02270329 \n```\n:::\n:::\n\n\n## Example: *Challenger* disaster\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-15_80d334e5b9e874bb819da9a11d1cc0cc'}\n\n```{.r .cell-code}\nbase_res <- glm((damaged > 0) ~ 1, data = orings, family = binomial)\nanova(orings_res, base_res, test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: (damaged > 0) ~ temperature\nModel 2: (damaged > 0) ~ 1\n  Resid. Df Resid. Dev Df Deviance     F   Pr(>F)   \n1        21     20.315                              \n2        22     28.267 -1   -7.952 7.952 0.004804 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n# Other hypothesis tests\n\n## Fisher's exact test\n\n*Agresti (1990): A British woman claimed to be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first. The null hypothesis is that there is no association between the true order of pouring and the woman's guess. The alternative that there is a positive association (that the odds ratio is greater than 1).*\n\n## Fisher's exact test\n\n\n::: {.cell hash='25_advanced-statistics_cache/revealjs/unnamed-chunk-16_6561fcda363934724a5b19b2240aeebb'}\n\n```{.r .cell-code}\nTeaTasting <-\n  matrix(c(3, 1, 1, 3),\n         nrow = 2,\n         dimnames = list(Guess = c(\"Milk\", \"Tea\"),\n                         Truth = c(\"Milk\", \"Tea\")))\nfisher.test(TeaTasting, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  TeaTasting\np-value = 0.2429\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.3135693       Inf\nsample estimates:\nodds ratio \n  6.408309 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}