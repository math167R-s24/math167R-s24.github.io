{
  "hash": "b12b7c7ef79b2ae116687686a1725851",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Simulations\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n## Overview of today\n\n-   Random walks continued\n-   Timing code\n\n## Warm up: Estimating $\\pi$\n\nThe area of a radius $r$ circle is $\\pi r^2$. How could we use simulation to estimate $\\pi$?\n\n. . .\n\nHint: what if we randomly generated points on a square?\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggplot2)\nrandom_points <- data.frame(x = runif(500, -1, 1),\n                            y = runif(500, -1, 1))\nrandom_points <- random_points |>\n  mutate(in_circle = x^2 + y^2 < 1)\nggplot(random_points, aes(x = x, y = y, color = in_circle)) + \n  geom_point() +\n  theme(aspect.ratio=1)\n```\n:::\n\n\n## Warm up: Estimating $\\pi$\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/random_points_1_be2a74847236e929d65a7b8e9c1d14bd'}\n::: {.cell-output-display}\n![](19_more-simulations_files/figure-revealjs/random_points_1-1.png){width=960}\n:::\n:::\n\n\n## Warm up: Estimating $\\pi$\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/random_points_1c40bdcb31208d6f05ab0fa4ade97bec'}\n\n```{.r .cell-code}\nrandom_points <- data.frame(x = runif(500, -1, 1),\n                            y = runif(500, -1, 1))\nrandom_points <- random_points |>\n  mutate(in_circle = x^2 + y^2 < 1)\nrandom_points |> summarize(pi_est = 4 * mean(in_circle))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  pi_est\n1    3.2\n```\n:::\n:::\n\n\n## Random walks continued\n\nRecall the following code for a one-dimensional random walk:\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/random_walk_functional_0d4a02ebdc9407477601a20cc149f804'}\n\n```{.r .cell-code}\ntake_step <- function() {\n  # move forwards or backwards with equal probability\n  return(sample(c(1, -1), 1))\n}\nwalk_randomly <- function(n_steps, start = 0) {\n  if (n_steps <= 1) {\n    return(start)\n  }\n  x <- c(start, walk_randomly(n_steps - 1, start + take_step()))\n  return(x)\n}\nplot(1:100, walk_randomly(100), type = \"l\",\n     xlab = \"Step\", ylab = \"x\",\n     main = \"A one-dimensional random walk\")\n```\n\n::: {.cell-output-display}\n![](19_more-simulations_files/figure-revealjs/random_walk_functional-1.png){width=960}\n:::\n:::\n\n\n## Random walks continued\n\nWe can use this simulation code to estimate a number of properties about random walks, including:\n\n1.  If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?\n\n2.  If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?\n\n3.  The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?\n\n4.  What is the expected maximum value of $|x_i|$ for a random walk of length $n$?\n\n## Random walks continued\n\n*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*\n\nConsider that the results of `walk_randomly(n_steps)` is a vector of length `n_steps`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nwalk_randomly(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  0  1  2  3  2  3  2  1  0  1  2  1  0 -1  0 -1  0 -1  0  1\n```\n:::\n:::\n\n\n. . .\n\nWe need an expression to count the number of times we reach $x=0$ (minus the start).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsum(walk_randomly(20) == 0) - 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n:::\n\n\n## Random walks continued\n\n*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*\n\nNow we can use `replicate()` to repeatedly evaluate our expression and then take the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nmean(replicate(10000, sum(walk_randomly(20) == 0) - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.518\n```\n:::\n:::\n\n\n## Varying `n_steps` parameter\n\n*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/varying_n_steps_bd0e1c1fdb5536bbf842b6691df1ea18'}\n\n```{.r .cell-code}\nset.seed(123)\nns <- c(10, 20, 50, 100, 200, 500, 1000) \nexpected_returns <- \n  vapply(ns, \n         function(n_steps) \n           mean(replicate(1000, sum(walk_randomly(n_steps) == 0) - 1)),\n         numeric(1))\nplot(ns, expected_returns, type = \"l\", \n     xlab = \"Number of steps\", \n     ylab = \"Mean returns to x = 0\")\n```\n\n::: {.cell-output-display}\n![](19_more-simulations_files/figure-revealjs/varying_n_steps-1.png){width=960}\n:::\n:::\n\n\n## Random walks continued\n\nHow could we adapt this code to answer the question:\n\n*If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?*\n\nAs $n\\rightarrow\\infty$, how will the expected number of visits to $x=10$ before step $n$ change?\n\n## Random walks continued\n\n*If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?*\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/varying_n_steps_10_f6c0abfeeefb1ba320df04d7188ebdd0'}\n\n```{.r .cell-code}\nset.seed(123)\nns <- c(10, 20, 50, 100, 200, 500, 1000) \nexpected_returns <- \n  vapply(ns, \n         function(n_steps) \n           mean(replicate(1000, sum(walk_randomly(n_steps) == 10))),\n         numeric(1))\nplot(ns, expected_returns, type = \"l\", \n     xlab = \"Number of steps\", \n     ylab = \"Mean visits to x = 10\")\n```\n\n::: {.cell-output-display}\n![](19_more-simulations_files/figure-revealjs/varying_n_steps_10-1.png){width=960}\n:::\n:::\n\n\n\n\n## Expected time of return\n\nCan we adapt this code to answer the following question?\n\n*The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?*\n\nWe have to be a little clever here--we only ever compute a finite-length random walk.\n\nFor any nonnegative integer valued random variable $X$,\n\n$$E(X)=\\sum_{n = 1}^\\infty P(X>n)$$\n\n## Expected time of return\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/expected_return_e0b03f872ac91b0399db77cd9b339890'}\n\n```{.r .cell-code}\nset.seed(123)\nns <- 1:20\n# for each n, estimate the proportion of random walks that do NOT return\n# to zero, using simulations\nproportion_no_returns <- \n  vapply(ns, \n         function(n_steps) \n           mean(replicate(2500, sum(walk_randomly(n_steps) == 0) < 2)),\n         numeric(1))\nplot(ns, proportion_no_returns, type = \"l\", \n     xlab = \"Number of steps\", \n     ylab = \"Mean visits to x = 10\")\n```\n\n::: {.cell-output-display}\n![](19_more-simulations_files/figure-revealjs/expected_return-1.png){width=960}\n:::\n\n```{.r .cell-code}\nproportion_no_returns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.0000 1.0000 0.5080 0.4828 0.3720 0.3704 0.3308 0.3036 0.2836 0.2700\n[11] 0.2336 0.2504 0.2136 0.2340 0.2140 0.2084 0.1944 0.2196 0.1944 0.1784\n```\n:::\n:::\n\n\n## Expected time of return\n\nDoes the infinite series converge?\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/partial_sums_71691a43050c950e761df644b5cf4b6f'}\n\n```{.r .cell-code}\n# calculate partial sums of the infinite series\ncumsum(proportion_no_returns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.000 2.000 2.503 3.017 3.413 3.806 4.142 4.452 4.694 4.985 5.239 5.471\n[13] 5.708 5.924 6.131 6.350 6.538 6.728 6.906 7.092\n```\n:::\n:::\n\n\n## Expected time of return\n\n*The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?*\n\nIt turns out the expected length of time is infinite--a key example of why we have to be careful about using simulations to replace mathematical analysis.\n\n## Exercise\n\nHow would we write a simulation to answer the following question:\n\n*What is the expected maximum value of $|x_i|$ for a random walk of length $n$?*\n\n## Timing code\n\nWhen running simulations, the number of different dimensions we consider (number of simulations, number of steps, etc.) can rapidly increase computational cost.\n\nWe often may want to be able to estimate the runtime of our simulations or to compare the speed of various versions of a simulation.\n\n**Example**: Which is faster: vectorized code or a for loop?\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/example_vec_d0d1a847506d53fe2ba1c85dfd2aeab2'}\n\n```{.r .cell-code}\n# vectorized\nx_vec <- rnorm(100000) + 1\n\n# for loop\nx_for <- numeric()\nfor (i in 1:100000){\n  x_for[i] <- rnorm(1) + 1\n}\n```\n:::\n\n\n## `Sys.time()`\n\nThe `Sys.time()` function can be used to report the time at evaluation.\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/systime_b17f2a3ccd2e01b39ecdc0467e6b8532'}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\nx_vec <- rnorm(100000) + 1\nprint(Sys.time() - start_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0.003111124 secs\n```\n:::\n\n```{.r .cell-code}\nstart_time <- Sys.time()\nx_for <- numeric()\nfor (i in 1:100000){\n  x_for[i] <- rnorm(1) + 1\n}\nprint(Sys.time() - start_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0.1064081 secs\n```\n:::\n:::\n\n\n## `system.time()`\n\nAltenatively, the `system.time()` function can be used to time the evaluation of a specific expression,\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/systemtime_977eac58085758a1728947867391db48'}\n\n```{.r .cell-code}\nsystem.time(x_vec <- rnorm(100000) + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.003   0.000   0.002 \n```\n:::\n\n```{.r .cell-code}\nx_for <- numeric()\nsystem.time(\n\n  for (i in 1:100000){\n    x_for[i] <- rnorm(1) + 1\n  }\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.097   0.010   0.107 \n```\n:::\n:::\n\n\n## `microbenchmark`\n\nFinally, the `microbenchmark` package can be used to time functions repeatedly.\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/microbenchmark_c69061fbf902405c0c566885606fc9e6'}\n\n```{.r .cell-code}\n# vectorized\nsq_vectorized <- function(x) {\n    return(x ^ 2)\n}\n# for loop\nsq_for_loop <- function(x) {\n    for (i in 1:length(x)) {\n        x[i] <- x[i] ^ 2\n    }\n    return(x)\n}\n\nlibrary(microbenchmark)\n# microbenchmark to compare the two functions\nx <- rnorm(1000)\ncompare_sq <- microbenchmark(sq_vectorized(x),\n                             sq_for_loop(x),\n                             times = 100)\ncompare_sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: nanoseconds\n             expr   min    lq     mean median    uq     max neval\n sq_vectorized(x)   533   656  5058.58    697   738  432058   100\n   sq_for_loop(x) 26035 26199 39816.33  26240 26322 1328687   100\n```\n:::\n:::\n\n\n## Algorithmic efficiency\n\nThe amount of time code takes is related to the efficiency of the underlying algorithm.\n\nAlgorithms are commonly analyzed in terms of their asymptotic complexity. The most common way to describe complexity is Big O notation, where an algorithm with $O(g(n))$ complexity has a time requirement which is asymptotically proportional to $g(n)$.\n\nExamples:\n\n-   Finding the median in a sorted list of numbers is $O(1)$ constant complexity.\n\n-   Finding the largest number in an unsorted list of numbers is $O(n)$ linear complexity.\n\nIn practice, timing code can be used to examine these relationships\n\n## Example: inverting matrices.\n\nMatrix inversion is typically said to be $O(n^3)$.\n\n\n::: {.cell hash='19_more-simulations_cache/revealjs/inversion_b1ea5e05c9032bbdb9c10f03abbca063'}\n\n```{.r .cell-code}\ncompare_mat <- microbenchmark(mat_3_x_3 = solve(matrix(rnorm(9), nrow = 3)),\n                              mat_5_x_5 = solve(matrix(rnorm(25), nrow = 5)),\n                              mat_7_x_7 = solve(matrix(rnorm(49), nrow = 7)),\n                              mat_9_x_9 = solve(matrix(rnorm(81), nrow = 9)),\n                              times = 100)\ncompare_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n      expr    min      lq     mean median      uq      max neval\n mat_3_x_3  6.642  6.9700  7.26356  7.093  7.2570   21.689   100\n mat_5_x_5  7.667  7.9745  8.50668  8.241  8.7125   15.580   100\n mat_7_x_7  8.733  9.1020 48.84822  9.307  9.8195 3936.410   100\n mat_9_x_9 10.250 10.8035 11.52756 11.193 11.9310   16.236   100\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}