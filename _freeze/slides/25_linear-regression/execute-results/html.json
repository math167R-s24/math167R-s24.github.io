{
  "hash": "8aff026a7322fd83e45f2e7682d2f3f8",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Linear regression\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: source\n---\n\n\n## Overview of today\n\n- Review of linear regression \n- Why use a linear model?\n- Pitfalls of linear regression: breaking assumptions and more\n\n\n## Simple linear regression\n\nHow does the speed of a car affect its stopping distance?\n\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cars_cf1fcb4e04c2dfb08cc6467a03975001'}\n\n```{.r .cell-code}\nplot(dist ~ speed, data = cars,\n     xlab = \"Speed (mph)\",\n     ylab = \"Stopping Distance (ft)\",\n     pch  = 16, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/cars-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## Simple linear regression\n\nSuppose $x_1,\\ldots, x_n$ represent the `speed` values of each car in our dataset. Let $Y_1,\\ldots, Y_n$ represent the `dist` variable.\n\nThe **simple linear regression model** is given by \n$$Y_i=\\beta_0+\\beta_1x_i+\\epsilon_i$$\nwhere:\n-   $\\beta_0$ is the intercept parameter\n-   $\\beta_1$ is the slope parameter\n-   $\\epsilon_i$ represents iid $N(0,\\sigma^2)$ errors, where $\\sigma^2$ is the variance of $\\epsilon_i$.\n\n## Simple linear regression\n\nUnder this model, we can also write that\n$$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\nAs a result\n$$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n$$\\mathrm{Var}(Y_i\\mid X_i= x_i)= \\sigma^2$$\n\n## Simple linear regression\n\nTypically, we say there are four assumptions for simple linear regression:\n\n- **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n- **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent of each other.\n- **Normality**: $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n- **Equal variance**: The variance of the errors $\\epsilon_i$ is the same for all values of $x_i$.\n\nNote that even if one or more of these assumptions breaks, linear regression can still be useful if used with caution.\n\n## Simple linear regression: Least squares\n\nTypically, we start with data $Y_1,\\ldots, Y_n$ and $x_1,\\ldots, x_n$. How do we estimate $\\beta_0$ and $\\beta_1$?\n\nIn other words, what is the best line to use for modeling the relationship between $X$ and $Y$?\n\nUsually, we use the least squares line, which is the solution to the following optimization problem:\n$$\\arg\\min\\sum_{i=1}^n (y_i-(\\beta_0+\\beta_1x_i))^2=\\arg\\min\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$\nwhere $\\hat{y}_i$ is the model prediction for $y_i$.\n\n## Regression methods\n\nAssume we have some data:\n\n-   $X_1,\\ldots, X_p$: $p$ **independent variables/explanatory variables/covariates/predictors**\n-   $Y$: the **dependent variables/response/outcome**.\n\nWe want to know the relationship between our covariates and our response, we can do this with a method called **regression**. Regression provides us with a statistical method to conduct inference and prediction.\n\n## Regression methods\n\n-   **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance\n-   What is the relationship between sleep and GPA?\n-   Is parents' education or parents' income more important for explaining income?\n-   **prediction:** predict new/future outcomes from new/future covariates\n-   Can we predict test scores based on hours spent studying?\n\n## Exercise\n\nClassify the following questions as **inference** or **prediction**:\n\n1. Can we use high school students' SAT/ACT scores to estimate their college GPAs?\n2. Is there an association between political candidates' heights and the number of votes they receive?\n3. Do states with higher minimum wages have lower poverty rates?\n\n## Why linear regression?\n\nRegression methods focus on modeling the relationship between response $Y$ and explanatory variables $X_1,\\ldots, X_p$. \n\nLinear regression proposes a model of the form\n$$Y=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon; \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\nSometimes, the following weaker form is used (why is this weaker?)\n$$E(Y\\mid X_1,\\ldots, X_p)=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p$$\nIn other words, the expected value of $Y$ (given $X_1,\\ldots, X_p$) is a linear transformation of $X_1,\\ldots, X_p$.\n\n## Other models\n\nHeight/gravity\n\n\n## Other models: use cases\n\n\n\n## Why linear regression?\n\n1. **Simplicity**: Easy to fit, usually gets the general trend correct, hard to **overfit**\n2. **Interpretability**: Consider the following log-normal model:\n$$Y=\\exp(\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon); \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\nHow do we interpret $\\beta_1$? \n\n## Additivity\n\nNote that linear regression can often be generalized to encompass other models. Consider the following model:\n$$Y=\\beta_0+\\beta_1X_1 +\\beta_2X_1^2$$\nThis is an example of polynomial regression, but the methods for fitting this model are essentially the same as ordinary least squares, using $X_1^2$ as an extra covariate.\n\nMany alternative models can be written in this form by transforming the covariates or transforming the response $Y$.\n\n## Breaking the assumptions\n\nWhat happens if we break the standard assumptions of linear regression? Recall the assumptions:\n\n- **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n- **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent of each other.\n- **Normality**: $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n- **Equal variance**: The variance of the errors $\\epsilon_i$ is the same for all values of $x_i$.\n\nWe can observe what happens when we break each assumption in terms of our ability to conduct **inference** and **prediction**.\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"60%\"}\n::: {.smaller}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_ex_dummy_c7c543a09e1c4915e1680e319c5e830e'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_ex_f01b08d703833c09007bc1a680b418ed'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/lm_ex-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: {.smaller}\n**Inference**: Asymptotically valid confidence intervals and hypothesis tests for $\\beta_0$ and $\\beta_1$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_res_inf_010148342524591a96f8bdbe848495bb'}\n\n```{.r .cell-code}\nsummary(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9766 -0.5870 -0.1388  0.5345  1.8733 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.2927     0.2728   4.738    3e-05 ***\nx             1.9568     0.1006  19.448   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8904 on 38 degrees of freedom\nMultiple R-squared:  0.9087,\tAdjusted R-squared:  0.9063 \nF-statistic: 378.2 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n:::\n:::\n\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: {.smaller}\n**Prediction**: Assuming the model holds for new data, unbiased point predictions and asymptotically valid prediction intervals for $Y_{n+1}$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_res_pred_8d1bb5efef71c4a06dea5efccae1c1a5'}\n\n```{.r .cell-code}\npredict(lm_res, \n        newdata = list(x = 1), \n        interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.249552 1.404936 5.094169\n```\n:::\n:::\n\n:::\n:::\n\n## Breaking linearity\n\nWhat if $E(Y_i\\mid X_i= x_i)\\not= \\beta_0+\\beta_1x_i$?\n\n\n::: columns\n::: {.column width=\"60%\"}\n::: {.smaller}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cubic_ex_dummy_757dc37910d5695f00f0d40f5fea0073'}\n\n```{.r .cell-code}\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x^3)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cubic_ex_653e72928569cefd5d91a69bdba9dae6'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/cubic_ex-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n\n## Breaking linearity\n\nOur standard inference and prediction strategies no longer work in general.\n\n**Inference**: Confidence intervals and hypothesis tests for $\\beta_0$ and $\\beta_1$ are no longer valid.\n\n**Prediction**: Point predictions are no longer unbiased and prediction intervals are no longer valid.\n\n\n## Breaking normality of errors\n\nWhat if $E(Y_i\\mid X_i= x_i)\\not= \\beta_0+\\beta_1x_i$?\n\n\n::: columns\n::: {.column width=\"60%\"}\n::: {.smaller}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/norm_ex_dummy_ab610527b210d9fa70147c47ce1bb7a7'}\n\n```{.r .cell-code}\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x^3)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/norm_ex_5c84b0fb02c4ef06d3fea6ed2478187e'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/norm_ex-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Simple linear regression: the `lm` function\n\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cars2_a10bbc9272246125859a03caefa04196'}\n\n```{.r .cell-code}\nlm_res = lm(dist ~ speed, data = cars)\nlm_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n```\n:::\n:::\n\n\n## Simple linear regression: the `lm` function\n\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cars3_76cae1327833426428fe5964a4fdebdb'}\n\n```{.r .cell-code}\nplot(dist ~ speed, data = cars,\n     xlab = \"Speed (mph)\",\n     ylab = \"Stopping Distance (ft)\", \n     pch  = 16, col  = \"blue\")\nabline(lm_res,  col = \"red\")\n```\n\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/cars3-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## Multiple linear regression\n\nWhat if we have multiple input variables?\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-1_27fec7554c1e079c568769813da8285b'}\n\n```{.r .cell-code}\ndata(mtcars)\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n:::\n\n\n## Linear Regression\n\nGiven our response $Y$ and our predictors $X_1, \\ldots, X_p$, a **linear regression model** takes the form:\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n**Note:** If we wish to include a categorical covariate, we can add indicator variables for each category.\n\n## Linear Regression\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n-   $Y$: dependent variable, outcome, response\n-   $X_j$: independent variable, covariate, predictor\n-   $\\beta_0$: Intercept\n-   $\\beta_j$: coefficient, the expected difference in the response between two observations differing by one unit in $X_j$, with all other covariates identical.\n-   $\\epsilon$: error, noise, with mean $0$ and variance $\\sigma^2$\n\n## Linear Regression\n\nWe can fully write out a linear regression model\n\n$$\n\\begin{equation}\n\\begin{bmatrix} y_1 \\\\ y_2\\\\ \\vdots \\\\ y_n \\end{bmatrix} = \n\\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n1 & x_{2,1} & \\cdots & x_{2, k}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & \\cdots & x_{n, k}\\end{bmatrix}\n\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} +\n\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\n\\end{equation}\n$$\n\n## Matrix Notation\n\nThis can also be expressed in matrix form:\n\n$$\n\\begin{align}\n\\mathbf{Y} &= \\mathbf{X}\\beta + \\epsilon,\\\\\n\\epsilon &\\sim N(0,1)\n\\end{align}\n$$\n\n-   $\\mathbf{Y} \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of the response\n-   $\\mathbf{X} \\in \\mathbb{R}^{n \\times (k+1)}$: a $((k+1)\\times n)$ matrix of the predictors (including intercept)\n-   $\\beta \\in \\mathbb{R}^{((k+1)\\times 1)}$: a $(k+1)$-dimensional vector of regression parameters\n-   $\\epsilon \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of the error term\n\n## $\\large \\epsilon$: Error term\n\n$\\epsilon$, pronounced epsilon, represents the **error term** of our model. We can model $Y$ as a linear function of the $X$'s, but in the real world, the relationship won't always be perfect. There is noise! It can come from\n\n-   Measurement error in the $X$'s\n-   Measurement error in the $Y$'s\n-   Unobserved/missing variables in the model\n-   Deviations in the true model from linearity\n-   True randomness\n\nIn linear regression, we assume that this error term is normally distributed with mean zero and variance $\\sigma^2$.\n\n## $\\beta_0$: Intercept\n\n$\\beta_0$ is the **intercept term** of our model. Notice that\n\n$$\\mathbb{E}[Y|X_1 = X_2 = \\cdots = X_p = 0] = \\beta_0$$\n\nThus, $\\beta_0$ is the expected value of our response if all the covariates are equal to $0$. This is also known as the y-intercept of our model.\n\n## $X_j$: Independent variable\n\n$X_j$ represents the $j$<sup>th</sup> independent variable in our model. Notice that $$\\mathbb{E}[Y|X_1,\\ldots, X_p] = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p$$ What happens to this expectation if we increase $X_j$ by 1 unit, holding everything else constant?\n\nThe conditional expectation of $Y$ increases by $\\beta_j$.\n\n## $\\beta_j$: Coefficient\n\n$\\beta_j$ represents the $j$<sup>th</sup> regression coefficient in our model. From the previous slide, we saw that for every 1 unit increase in $X_j$, holding all other variables constant, the expected value of the response increases by $\\beta_j$. From this we can derive an interpretation.\n\n**Interpretation of** $\\beta_j$: the expected difference in the response between two observations differing by one unit in $X_j$, with all other covariates identical.\n\n## `lm()`: Linear Model\n\nWe fit a linear regression model in R using `lm()`. The first argument is a **formula**, which is a type of R object. Formulas typically take the following form: `Y ~ X_1 + X_2 + ... + X_p`.\n\nThe dependent variable, `Y` goes on the left-hand side of the tilde `~`, which marks the formula. The independent variables are added on the right-hand side. Using this formula will give us a model in the form of\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n\n::: {.cell}\n\n:::\n\n\n## `lm()`: Linear Model\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-3_4e1f9bf3dd97dbf2dc12e52ee4c6c4b1'}\n\n```{.r .cell-code}\ndata(mtcars)\nmy_lm <- lm(mpg ~ hp + wt, data = mtcars)\nclass(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"lm\"\n```\n:::\n\n```{.r .cell-code}\nmy_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nCoefficients:\n(Intercept)           hp           wt  \n   37.22727     -0.03177     -3.87783  \n```\n:::\n:::\n\n\n## `lm()`: Linear Model\n\nWe can see from `names()` that `lm` objects contain a lot more than they print out by default.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-4_f432314a05f3c3a3920c0705a2011e1d'}\n\n```{.r .cell-code}\nnames(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n:::\n:::\n\n\n## `summary()` Model summaries\n\n`summary()` or `summary.lm()` gives us a summary of our `lm` object in R.\n\n-   The quantiles of the residuals: hopefully, they match a normal distribution.\n-   Coefficients, their standard errors, and their individual significances\n-   (Adjusted) R-squared value: how much of the overall variability in the response is explained by the model?\n-   F-statistic: hypothesis test for the significance of the overall model\n\n## `summary()` Model summaries\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-5_6b5ee7bbff9e0ebe4e9c001aa7c7a4ed'}\n\n```{.r .cell-code}\nsummary(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n```\n:::\n:::\n\n\n## `plot()`: Regression model diagnostics\n\nCalling `plot(my_lm)` will return several diagnostic plots. Remember that we want our error term to look normally distributed with mean zero. We won't go into all the details for this class, but here are some tips:\n\n-   **Residuals vs Fitted:** these are your errors (residuals) plotted over the predicted outcome (fitted). Errors should be random, so here you want to see randomly scattered points with no discernable pattern. You want the trend line to be approximately horizontal.\n-   **Normal Q-Q plot:** These are the quantiles of your errors against the quantiles of a normal distribution. In theory, your errors should be normally distributed, so you are hoping that points are mostly along the 45-degree $y=x$ line.\n\n## `plot()`: Regression model diagnostics\n\n-   **Scale-location:** This looks at the magnitude of standardized residuals over the predicted outcome. Similar interpretation as residuals vs fitted. This can make it slightly easier to identify undesireable patterns.\n-   **Residuals vs leverage:** This can help identify highly influential points, such as outliers. If points are outside dotted red lines, then removing them would noticeably alter your results. *Never just remove outliers!* If it's real data, it's valid and removing it will bias your results. It is much more important to understand why outliers are there than to remove them.\n\n## `plot()`: Regression model diagnostics\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-6_98bc54d71db19e307923d821542d3a6f'}\n\n```{.r .cell-code}\nplot(my_lm)\n```\n:::\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-7_c442a483422082b59900ada53089ebe5'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## `plot()`: Regression model diagnostics\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-8_6c232d223d8b4832afe36d326be599ef'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## `plot()`: Regression model diagnostics\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-9_3e5695ed2dee905f2c976e7d01b3c93c'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## `plot()`: Regression model diagnostics\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-10_ba17c8a882b42dfa6d66dbff6fefdba4'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## `coef()`: Extract coefficients\n\nUse `coef()` to extract estimated coefficients as a vector.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-11_b2a5252a0c146c3ec06a4dea8936b7fc'}\n\n```{.r .cell-code}\ncoef(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          hp          wt \n37.22727012 -0.03177295 -3.87783074 \n```\n:::\n:::\n\n\n## `fitted()` Extract fitted values\n\nUse `fitted()` to extract the fitted/estimated values for the response. This can be useful to compare how our fitted values compare to the estimated values to help assess model fit.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-12_11e58b66c0866bdcd46ff1d2cc6b1248'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nmod_fits <- fitted(my_lm)\nmy_df <- data.frame(actual = mtcars$mpg, fitted = mod_fits)\nggplot(my_df, aes(x = fitted, y = actual)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, col = \"red\", lty = 2) + \n  theme_bw(base_size = 15) +\n  labs(x = \"Fitted values\", y = \"Actual values\", title = \"Actual vs. Fitted\") +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n:::\n\n\n## `fitted()` Extract fitted values\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-13_d678d74763ff5407b0b4e26a936fe3f8'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## `predict()`: Predict new outcomes\n\nUse `predict()` to predict new outcomes given new explanatory variables. For example, pretend we observe two new cars with horsepowers of `100` and `150`, respectively, and weights of `3000` and `3500`, respectively.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-14_39e4f47d74c1069a84fd6f2789e3f772'}\n\n```{.r .cell-code}\n# Note: wt is in 1000s of lbs\nnew_obs <- data.frame(hp = c(100, 150), wt = c(3, 3.5))\npredict(my_lm, new_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2 \n22.41648 18.88892 \n```\n:::\n:::\n\n\nWe'll come back to prediction in future lectures.\n\n## `residuals()`: Compute residuals\n\nUse `residuals()` to compute the residuals for fitted values.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-15_b6a9614ebc057bc3fadf0fe17cb4ccde'}\n\n```{.r .cell-code}\nresiduals(my_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n        -2.57232940         -1.58348256         -2.47581872          0.13497989 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n         0.37273336         -2.37381631         -1.29904236          1.51293266 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n         0.80632669         -0.77945988         -2.17945988          0.67463146 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n         0.25616901         -1.64993945          0.04479541          1.03726743 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n         5.50751301          5.80097202          1.08761978          5.85379085 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n        -3.08644148         -3.31136386         -3.94097947         -1.25202805 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n         2.44325481         -0.32665313         -0.03737415          2.63023081 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n        -0.74648866         -1.22541324          2.26052287         -1.58364943 \n```\n:::\n:::\n\n\n## Manipulating formulas\n\nWorking with formulas in R can be somewhat confusing, so it is important to understand how formulas work to make sure you are fitting the intended model.\n\n## `- 1`\n\nUse `- 1` to remove an intercept from your model. Only do this if you are very sure that what you are doing is appropriate. I don't recommend doing this in practice.\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-16_47875455c2f1600c4f0a5d2d27a185bf'}\n\n```{.r .cell-code}\nno_intercept <- lm(mpg ~ hp + wt - 1, data = mtcars)\nsummary(no_intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp + wt - 1, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.407  -2.382   2.511   7.091  23.885 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)   \nhp -0.03394    0.03940  -0.861   0.3959   \nwt  6.84045    1.89425   3.611   0.0011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.32 on 30 degrees of freedom\nMultiple R-squared:  0.7264,\tAdjusted R-squared:  0.7082 \nF-statistic: 39.83 on 2 and 30 DF,  p-value: 3.599e-09\n```\n:::\n:::\n\n\n## `:` operator\n\nUse `X1:X2` to include an interaction effect in your model. This is useful if you have reason to believe two covariates interact, such as gender and education in a wage model. In our case, we'll assume horsepower and weight interact in their effect on mpg.\n\nTypically (always?), if you include an interaction effect, you should also include the marginal effects. You can do this automatically using `X1*X2`.\n\n## `:` operator\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-17_6a52f8797e94a297ec44abe41af9e8dc'}\n\n```{.r .cell-code}\ninteract <- lm(mpg ~ hp:wt, data = mtcars)\nsummary(interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp:wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8831 -2.0952 -0.4577  1.2262  7.9282 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.745642   1.062236   26.12  < 2e-16 ***\nhp:wt       -0.014872   0.001727   -8.61 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.288 on 30 degrees of freedom\nMultiple R-squared:  0.7119,\tAdjusted R-squared:  0.7023 \nF-statistic: 74.14 on 1 and 30 DF,  p-value: 1.321e-09\n```\n:::\n:::\n\n\n## `*` operator\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-18_a1a50dc0f7ec0a05b8a914697dc6b9d5'}\n\n```{.r .cell-code}\ninteract <- lm(mpg ~ hp*wt, data = mtcars)\nsummary(interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp:wt        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,\tAdjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n```\n:::\n:::\n\n\n## `.` operator\n\nUse `~ .` to include all non-response variables in the input data as independent variables.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869,\tAdjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n```\n:::\n:::\n\n\n## Estimation\n\nHow do we choose/estimate $\\beta_{(k+1)\\times1}$?\n\nLeast squares finds the line that minimizes the squared distance between the points and the line, i.e. makes $$\\left[y_i - (\\beta_0 + \\beta_1 x_{i, 1} + \\dots + \\beta_k x_{i,k})\\right]^2$$ as small as possible for all $i = 1, \\dots, n$.\n\nThe vector $\\widehat{\\beta}$ that minimizes the sum of the squared distances is ()\n\n$$ \\widehat{\\beta}=\\left(\\mathbf{X}^T \\mathbf{X} \\right)^{-1}\\mathbf{X}^T \\mathbf{Y}.$$\n\nNote: In statistics, once we have estimated a parameter we put a \"hat\" on it, e.g. $\\widehat{\\beta_0}$ is the estimate of the true parameter $\\beta_0$.\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the following simulation:\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-20_07bfe66ec759a985d1298f3bf9d80041'}\n\n```{.r .cell-code}\nn <- 50\nx1 <- rnorm(n)\nx2 <- x1 * 2\ne <- rnorm(n)\ny <- 1 + 3 * x1 + e\nlm(y ~ x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1)\n\nCoefficients:\n(Intercept)           x1  \n      1.146        2.814  \n```\n:::\n\n```{.r .cell-code}\nlm(y ~ x1 + x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nCoefficients:\n(Intercept)           x1           x2  \n      1.146        2.814           NA  \n```\n:::\n:::\n\n\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the following simulation:\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-21_8d0d6f798f4d2511dd5a8f74c4f5707f'}\n\n```{.r .cell-code}\nx2b <- x1 * 2 + rnorm(n, sd = .01)\nlm(y ~ x1 + x2b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2b)\n\nCoefficients:\n(Intercept)           x1          x2b  \n      1.114      -44.331       23.580  \n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}