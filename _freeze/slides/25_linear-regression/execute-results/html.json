{
  "hash": "c19a9763725e5941ffdbd6ed744aeb03",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Linear regression\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: source\n---\n\n\n## Overview of today\n\n- Review of linear regression \n- Why use a linear model?\n- Pitfalls of linear regression: breaking assumptions and more\n\n## Warm-up\n\nWhat is the method of least squares and how does it relate to linear regression?\n\n\n\n\n\n\n## Simple linear regression\n\nHow does the speed of a car affect its stopping distance?\n\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cars_cf1fcb4e04c2dfb08cc6467a03975001'}\n\n```{.r .cell-code}\nplot(dist ~ speed, data = cars,\n     xlab = \"Speed (mph)\",\n     ylab = \"Stopping Distance (ft)\",\n     pch  = 16, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/cars-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## Simple linear regression\n\nSuppose $x_1,\\ldots, x_n$ represent the `speed` values of each car in our dataset. Let $Y_1,\\ldots, Y_n$ represent the `dist` variable.\n\nThe **simple linear regression model** is given by \n$$Y_i=\\beta_0+\\beta_1x_i+\\epsilon_i$$\n-   $\\beta_0$ is the intercept parameter\n\n-   $\\beta_1$ is the slope parameter\n\n-   $\\epsilon_i$ represents iid $N(0,\\sigma^2)$ errors, where $\\sigma^2$ is the variance of $\\epsilon_i$.\n\n## Simple linear regression\n\nUnder this model, we can also write that\n$$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\nAs a result\n$$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n$$\\mathrm{Var}(Y_i\\mid X_i= x_i)= \\sigma^2$$\n\n## Simple linear regression\n\nTypically, we say there are four assumptions for simple linear regression:\n\n- **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n- **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent of each other.\n- **Normality**: $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n- **Equal variance**: The variance of the errors $\\epsilon_i$ is the same for all values of $x_i$.\n\nNote that even if one or more of these assumptions breaks, linear regression can still be useful if used with caution.\n\n## Simple linear regression: Least squares\n\nTypically, we start with data $Y_1,\\ldots, Y_n$ and $x_1,\\ldots, x_n$. How do we estimate $\\beta_0$ and $\\beta_1$?\n\nIn other words, what is the best line to use for modeling the relationship between $X$ and $Y$?\n\nUsually, we use the least squares line, which is the solution to the following optimization problem:\n$$\\arg\\min\\sum_{i=1}^n (y_i-(\\beta_0+\\beta_1x_i))^2=\\arg\\min\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$\nwhere $\\hat{y}_i$ is the model prediction for $y_i$.\n\n## Regression methods\n\nAssume we have some data:\n\n-   $X_1,\\ldots, X_p$: $p$ **independent variables/explanatory variables/covariates/predictors**\n-   $Y$: the **dependent variables/response/outcome**.\n\nWe want to know the relationship between our covariates and our response, we can do this with a method called **regression**. Regression provides us with a statistical method to conduct inference and prediction.\n\n## Regression methods\n\n-   **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance\n-   What is the relationship between sleep and GPA?\n-   Is parents' education or parents' income more important for explaining income?\n-   **prediction:** predict new/future outcomes from new/future covariates\n-   Can we predict test scores based on hours spent studying?\n\n## Exercise\n\nClassify the following questions as **inference** or **prediction**:\n\n1. Can we use high school students' SAT/ACT scores to estimate their college GPAs?\n2. Is there an association between political candidates' heights and the number of votes they receive?\n3. Do states with higher minimum wages have lower poverty rates?\n\n## Why linear regression?\n\nRegression methods focus on modeling the relationship between response $Y$ and explanatory variables $X_1,\\ldots, X_p$. \n\nLinear regression proposes a model of the form\n$$Y=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon; \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\nSometimes, the following weaker form is used (why is this weaker?)\n$$E(Y\\mid X_1,\\ldots, X_p)=\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p$$\nIn other words, the expected value of $Y$ (given $X_1,\\ldots, X_p$) is a linear transformation of $X_1,\\ldots, X_p$.\n\n## Other models\n\nHeight/gravity\n\n\n## Other models: use cases\n\n\n\n## Why linear regression?\n\n1. **Simplicity**: Easy to fit, usually gets the general trend correct, hard to **overfit**\n2. **Interpretability**: Consider the following log-normal model:\n$$Y=\\exp(\\beta_0+\\beta_1X_1+\\cdots +\\beta_p X_p+\\epsilon); \\hspace{1em} \\epsilon\\sim N(0, \\sigma^2)$$\nHow do we interpret $\\beta_1$? \n\n## Additivity\n\nNote that linear regression can often be generalized to encompass other models. Consider the following model:\n$$Y=\\beta_0+\\beta_1X_1 +\\beta_2X_1^2$$\nThis is an example of polynomial regression, but the methods for fitting this model are essentially the same as ordinary least squares, using $X_1^2$ as an extra covariate.\n\nMany alternative models can be written in this form by transforming the covariates or transforming the response $Y$.\n\n## Assumptions of linear regression\n\n\n- **Linearity**: The relationship between $X$ and $E(Y\\mid X)$ is linear: $$E(Y_i\\mid X_i= x_i)= \\beta_0+\\beta_1x_i$$\n- **Independence**: The observations $Y_1,\\ldots, Y_n$ are independent of each other.\n- **Normality**: $$Y_i\\mid X_i= x_i\\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$$\n- **Equal variance**: The variance of the errors $\\epsilon_i$ is the same for all values of $x_i$.\n\n## Checking the assumptions\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"60%\"}\n::: {.smaller}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_ex_dummy_c7c543a09e1c4915e1680e319c5e830e'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_ex_f01b08d703833c09007bc1a680b418ed'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/lm_ex-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: {.smaller}\n**Inference**: Asymptotically valid confidence intervals and hypothesis tests for $\\beta_0$ and $\\beta_1$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_res_inf_010148342524591a96f8bdbe848495bb'}\n\n```{.r .cell-code}\nsummary(lm_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9766 -0.5870 -0.1388  0.5345  1.8733 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.2927     0.2728   4.738    3e-05 ***\nx             1.9568     0.1006  19.448   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8904 on 38 degrees of freedom\nMultiple R-squared:  0.9087,\tAdjusted R-squared:  0.9063 \nF-statistic: 378.2 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n:::\n:::\n\n\n## All assumptions true\n\n::: columns\n::: {.column width=\"45%\"}\n::: {.smaller}\n**Prediction**: Assuming the model holds for new data, unbiased point predictions and asymptotically valid prediction intervals for $Y_{n+1}$.\n:::\n:::\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/lm_res_pred_8d1bb5efef71c4a06dea5efccae1c1a5'}\n\n```{.r .cell-code}\npredict(lm_res, \n        newdata = list(x = 1), \n        interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.249552 1.404936 5.094169\n```\n:::\n:::\n\n:::\n:::\n\n## Breaking linearity\n\nWhat if $E(Y_i\\mid X_i= x_i)\\not= \\beta_0+\\beta_1x_i$?\n\n\n::: columns\n::: {.column width=\"60%\"}\n::: {.smaller}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cubic_ex_dummy_757dc37910d5695f00f0d40f5fea0073'}\n\n```{.r .cell-code}\nx <- runif(n = 40, min = 0, max = 5)\ny <- rnorm(n = 40, mean = 1 + 2 * x^3)\nlm_res <- lm(y ~ x)\nggplot(mapping = aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_abline(\n    slope = lm_res$coefficients[2], \n    intercept = lm_res$coefficients[1],\n    color = \"red\"\n  ) + \n  theme_minimal()\n```\n:::\n\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" hash='25_linear-regression_cache/revealjs/cubic_ex_653e72928569cefd5d91a69bdba9dae6'}\n::: {.cell-output-display}\n![](25_linear-regression_files/figure-revealjs/cubic_ex-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n\n## Breaking linearity\n\nOur standard inference and prediction strategies no longer work in general.\n\n**Inference**: Confidence intervals and hypothesis tests for $\\beta_0$ and $\\beta_1$ are no longer valid.\n\n**Prediction**: Point predictions are no longer unbiased and prediction intervals are no longer valid.\n\n\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the following simulation:\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-1_c31754b6599e570db593a6ce03fca37a'}\n\n```{.r .cell-code}\nn <- 50\nx1 <- rnorm(n)\nx2 <- x1 * 2\ne <- rnorm(n)\ny <- 1 + 3 * x1 + e\nlm(y ~ x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1)\n\nCoefficients:\n(Intercept)           x1  \n      1.031        3.120  \n```\n:::\n\n```{.r .cell-code}\nlm(y ~ x1 + x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nCoefficients:\n(Intercept)           x1           x2  \n      1.031        3.120           NA  \n```\n:::\n:::\n\n\n\n## Collinearity\n\nWhat if our predictor variables are closely related? Consider the following simulation:\n\n\n::: {.cell hash='25_linear-regression_cache/revealjs/unnamed-chunk-2_0819861faa532bca370a0f73ec436e5b'}\n\n```{.r .cell-code}\nx2b <- x1 * 2 + rnorm(n, sd = .01)\nlm(y ~ x1 + x2b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2b)\n\nCoefficients:\n(Intercept)           x1          x2b  \n      1.042       33.013      -14.939  \n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}