{
  "hash": "c3d0c420ea509c7ad92282348b29ba7b",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Statistical inference\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n## Overview of today\n\n-   Review of law of large numbers\n-   Review of central limit theorem\n-   Review of confidence intervals\n-   Computing confidence intervals in R\n-   Simulations with confidence intervals\n\n## Law of large numbers\n\nWe have been using simulations to estimate $E(Y)$ where $Y$ is some random variable. Why does this work?\n\n**Review:** The law of large numbers, informally speaking, states that if we perform the same experiment many times, the average of the observed results will be close to the expected value (and will get closer as we increase the number of trials).\n\n. . .\n\nThe LLN can be formalized as follows: if $X_1,\\ldots, X_n$ are iid random variables with expected value $\\mu$, then the sample mean $\\overline{X}_n$ will converge to $\\mu$ (in probability or almost surely).\n\n## Law of large numbers simulation\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/lln_46cd61f3c1620912e803864d86fd1d29'}\n\n```{.r .cell-code}\nset.seed(1028)\ndice_rolls <- sample(1:6, 1000, replace = TRUE)\nsample_avg <- sapply(1:length(dice_rolls), function(i) mean(dice_rolls[1:i]))\nplot(1:1000, sample_avg, type = \"l\", col = \"red\",\n     xlab = \"Rolls\", ylab = \"Average\")\nabline(h = 3.5, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/lln-1.png){width=960}\n:::\n:::\n\n\n## Central limit theorem\n\nWhat does the central limit theorem say?\n\n. . .\n\nLet $X_1,\\ldots, X_n$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$. Then if $n$ is sufficiently large, $\\overline{X}_n$ has approximately a normal distribution with\n\n-   $E(\\overline{X}_n)=\\mu$\n-   $\\mathrm{Var}(\\overline{X}_n)=\\sigma^2/n$.\n\nThe larger the value of $n$, the better the approximation. (Devore p. 232).\n\n## Central limit theorem\n\nSuppose $X_1,\\ldots, X_{10}\\sim N(2, 4)$:\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/clt1_bb46822ab503f33ad2e89366ad425a07'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nXbars <- data.frame(Xbar = replicate(1000, mean(rnorm(10, mean = 2, sd = 2))))\nggplot(Xbars, aes(x = Xbar)) + \n  geom_histogram(aes(y = ..density..)) + \n  geom_function(fun = dnorm, args = list(mean = 2, sd = 2 / sqrt(10)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/clt1-1.png){width=960}\n:::\n:::\n\n\n## Central limit theorem\n\nWhat if $n = 5$?\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/clt2_e0696dda450f9844de57f2b4685983b5'}\n\n```{.r .cell-code}\nn <- 5\nXbars <- data.frame(Xbar = replicate(1000, mean(rnorm(n, mean = 2, sd = 2))))\nggplot(Xbars, aes(x = Xbar)) + \n  geom_histogram(aes(y = ..density..)) + \n  geom_function(fun = dnorm, args = list(mean = 2, sd = 2 / sqrt(n)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/clt2-1.png){width=960}\n:::\n:::\n\n\n## Central limit theorem\n\nWhat if $n = 5$, but $X_1,\\ldots, X_{5}\\sim \\mathrm{Exp}(1)$:\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/clt3_e02bd24469ecc5e6c6373dfbd5602f3e'}\n\n```{.r .cell-code}\nn <- 5\nXbars <- data.frame(Xbar = replicate(1000, mean(rexp(n, rate = 1))))\nggplot(Xbars, aes(x = Xbar)) + \n  geom_histogram(aes(y = ..density..)) + \n  geom_function(fun = dnorm, args = list(mean = 1, sd = 1 / sqrt(n)), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/clt3-1.png){width=960}\n:::\n:::\n\n\n## Exercise\n\nHow can you break the central limit theorem?\n\n*Let* $X_1,\\ldots, X_n$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$. Then if $n$ is sufficiently large, $\\overline{X}_n$ has approximately a normal distribution with $E(\\overline{X}_n)=\\mu$ and $\\mathrm{Var}(\\overline{X}_n)=\\sigma^2/n$. The larger the value of $n$, the better the approximation. (Devore p. 232).\n\n. . .\n\n**What assumptions can you break?**\n\n## Confidence intervals\n\nLast class, we talked about bootstrap confidence intervals, but today we will return to the classic statistical theory for constructing confidence intervals.\n\n## A simple example: Normal population\n\nA simple case is for constructing confidence intervals for a normally distributed population variable. If we assume\n\n-   The population distribution is normal\n-   The value of the population standard deviation $\\sigma$ is known\n\nGiven samples $X_1,\\ldots, X_n$, we can compute $\\overline{X}_n$ and then\n\n$$\\frac{\\overline{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0, 1)$$\n\n## A simple example: Normal population\n\nWe can thus compute a **95% confidence interval**:\n\n$$P\\left(-1.96< \\frac{\\overline{X}_n-\\mu}{\\sigma/\\sqrt{n}}<1.96\\right)=.95$$\n\nAlgebraic manipulation yields\n\n$$P\\left(\\overline{X}_n-1.96\\frac{\\sigma}{\\sqrt{n}}<\\mu<\\overline{X}_n+1.96\\frac{\\sigma}{\\sqrt{n}}\\right)=.95$$\n\nThe final interval can be written as $\\overline{X}_n\\pm 1.96(\\sigma/\\sqrt{n})$.\n\n## A simple example: Normal population\n\nIn general, for a $100(1-\\alpha)\\%$ confidence interval, we can use\n\n$$P\\left(-z_{\\alpha/2}< \\frac{\\overline{X}_n-\\mu}{\\sigma/\\sqrt{n}}<z_{\\alpha/2}\\right)=1-\\alpha$$\n\nwhere $z_{\\alpha/2}$ is defined by $1 - P(Z\\leq z_{\\alpha/2})=\\alpha/2$. This yields the interval\n\n$$\\overline{X}_n\\pm z_{\\alpha/2}(\\sigma/\\sqrt{n})$$\n\n## Interpretation\n\nConsider the equation\n\n$$P\\left(\\overline{X}_n-1.96\\frac{\\sigma}{\\sqrt{n}}<\\mu<\\overline{X}_n+1.96\\frac{\\sigma}{\\sqrt{n}}\\right)=.95$$\n\n**Which elements of this equation are random?**\n\n. . .\n\nRemember, $\\overline{X}_n$ and thus the interval itself are random. Prior to observing the data and computing $\\overline{X}_n$, there is a .95 probability that the interval will include the true value of $\\mu$.\n\n. . .\n\nHowever, we cannot say anything about the probability that any particular confidence interval contains $\\mu$.\n\n## Example (Devore 7.1)\n\nA sample of $n=31$ trained typists was selected, and the preferred keyboard height was determined for each typist. The resulting sample average preferred height was $x=80.0$cm.\n\nAssuming that the preferred height is normally distributed with $\\sigma = 2.0$cm, obtain a 95% confidence interval (interval of plausible values) for $\\mu$, the true average preferred height.\n\n. . .\n\n$$\\overline{X}_n\\pm 1.96\\frac{2.0}{\\sqrt{31}}=80.0\\pm .7=(79.3, 80.7)$$\n\n## Simulating many confidence intervals\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/many.ci_b0f1bdbbde4a3d686c0454b232cdd87d'}\n\n```{.r .cell-code}\nn <- 30 # sample size\nmu <- 10 # unknown true mean\nsigma <- 2 # known standard deviation\nXbars <- replicate(100, mean(rnorm(n, mean = mu, sd = sigma)))\ncoverage_rate <- mean(abs(Xbars - mu) < 1.96 * sigma / sqrt(n))\ncoverage_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.92\n```\n:::\n:::\n\n\n## Simulating many confidence intervals\n\nCode to visualize all of our confidence intervals:\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/many.ci.vis_4225061cc12ed833bb4177c169a4395b'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nplot_dat <- data.frame(id = 1:100,\n                       Xbar = Xbars) |>\n  mutate(upper = Xbar + 1.96 * sigma / sqrt(n),\n         lower = Xbar - 1.96 * sigma / sqrt(n))\nggplot(plot_dat, \n       aes(x = id, y = Xbar, \n           color = abs(Xbars - mu) < 1.96 * sigma / sqrt(n))) + \n  geom_errorbar(aes(ymin = lower, ymax = upper)) +\n  geom_abline(intercept = mu, slope = 0) + \n  theme(legend.position = \"none\")\n```\n:::\n\n\n## Simulating many confidence intervals\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/many.ci.vis.run_022e46e9acd59c189fab42137e332877'}\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/many.ci.vis.run-1.png){width=960}\n:::\n:::\n\n\n## Large-sample confidence intervals\n\n**What if we do not know whether the population distribution is normal?** Provided $n$ is sufficiently large, the CLT implies that $\\overline{X}_n$ is approximately normal regardless of the population distribution.\n\n**What if we do not know** $\\sigma?$ Again, provided $n$ is sufficiently large, we can replace $\\sigma$ with the sample standard deviation $S$.\n\nWe obtain the confidence interval formula:\n\n$$\\overline{X}_n\\pm z_{\\alpha/2}(S/\\sqrt{n})$$\n\n## Simulating many confidence intervals\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/many.ci.pois_d4947f540479b839e4dc99971d2c3746'}\n\n```{.r .cell-code}\nn <- 30 # sample size\nlambda <- 10\nsample_stats_poisson <- function() {\n  x <- rpois(n, lambda)\n  return(c(mean(x), sd(x)))\n}\nstats <- replicate(100, sample_stats_poisson())\nstats[, 1:5] # print out first five simulation results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]     [,3]     [,4]      [,5]\n[1,] 10.300000 9.966667 9.800000 9.633333 10.066667\n[2,]  3.131046 3.699798 3.188341 2.846454  3.061815\n```\n:::\n\n```{.r .cell-code}\ncoverage_rate <- mean(abs(stats[1,] - mu) < 1.96 * stats[2,] / sqrt(n))\ncoverage_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.98\n```\n:::\n:::\n\n\n## When $n$ is small but $\\sigma$ unknown: $t$-intervals\n\n**What about when** $n$ is small, but $\\sigma$ is unknown? Assuming again that the population distribution is normal,\n\n$$T=\\frac{\\overline{X}_n-\\mu}{S/\\sqrt{n}}$$\n\nhas a $t$ distribution with $n-1$ degrees of freedom. We can thus use the confidence interval\n\n$$\\overline{X}_n\\pm t_{\\alpha/2, n-1}(S/\\sqrt{n})$$ where $P(T\\leq t_{p, n-1})=p$ when $T$ follows a $t$-distribution with $n-1$ degrees of freedom.\n\n## Assessing normality of the population distribution\n\nHow can we tell if the population distribution is normal? One simple check is to use a quantile-quantile (QQ) plot.\n\n1.  Using your data, estimate the quantiles using the empirical distribution.\n2.  Compare the **sample quantiles** with the quantiles of a known distribution (often normal).\n3.  If the sample and theoretical quantiles appear to fall on a line, then the empirical distribution function will be similar to (a scaled version of) the theoretical distribution function.\n\n## Example: Precipitation\n\nThe `precip` dataset includes annual precipitation in inches for a number of US cities. Is this normally distributed?\n\n\n::: {.cell hash='21_confidence-intervals_cache/revealjs/precip_e909d7dab9a9bb2878046aa2793b2d61'}\n\n```{.r .cell-code}\nhead(precip)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Mobile      Juneau     Phoenix Little Rock Los Angeles  Sacramento \n       67.0        54.7         7.0        48.5        14.0        17.2 \n```\n:::\n\n```{.r .cell-code}\nhist(precip)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/precip-1.png){width=960}\n:::\n:::\n\n\n## Example: Precipitation\n\nThe `qqnorm()` function can be used to compare the empirical/sample quantiles and the theoretical quantiles of a standard normal. The `qqline()` function adds a line that passes through the first and third quartiles.\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/precip.qq_15b5a06f5762f4a66ba3c979989d24d4'}\n\n```{.r .cell-code}\nqqnorm(precip, ylab = \"Precipitation [in/yr] for 70 US cities\", pch = 16)\nqqline(precip)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/precip.qq-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n## Example: Penguins\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/peng.qq_55d8903633b32baa4344d4c2e85e5067'}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\ndata(penguins)\nqqnorm(penguins$bill_length_mm, ylab = \"Penguin bill lengths\", pch = 16)\nqqline(penguins$bill_length_mm)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/peng.qq-1.png){fig-align='center' width=480}\n:::\n:::\n\n## Example: Penguins\n\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/peng.hist.2_24173412e5f78aef991413694992ba34'}\n\n```{.r .cell-code}\nhist(penguins$bill_length_mm)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/peng.hist.2-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n## Example: Penguins\n\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/peng.qq.2_8535d33522dcb70af41b57b4a1ad0dcf'}\n\n```{.r .cell-code}\nadelie <- penguins |> dplyr::filter(species == \"Adelie\")\nqqnorm(adelie$bill_length_mm, ylab = \"Penguin bill lengths\", pch = 16)\nqqline(adelie$bill_length_mm)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/peng.qq.2-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n## Example: Penguins\n\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/peng.hist.3_aead1bebe123dd928c54223a357eb436'}\n\n```{.r .cell-code}\nhist(adelie$bill_length_mm)\n```\n\n::: {.cell-output-display}\n![](21_confidence-intervals_files/figure-revealjs/peng.hist.3-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n## Using R to compute confidence intervals\n\nGenerally, if you are only doing a simple normal-based or $t$-based confidence interval for a mean, I recommend computing $\\overline{X}_n$ and $S$ using `mean()` and `sd()`, and then computing the confidence interval by hand.\n\nYou can also use the `t.test()` function to compute the appropriate $t$-interval for a population mean:\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/precip.ci_3c34f7f7f1467b4a20169fec8ba0b98d'}\n\n```{.r .cell-code}\nprecip_t <- t.test(precip, conf.level = 0.95)\nprecip_ci <- precip_t$conf.int # get confidence interval\nprecip_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31.61748 38.15395\nattr(,\"conf.level\")\n[1] 0.95\n```\n:::\n:::\n\n\n## Comparing $t$-based and bootstrap confidence intervals\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.8' hash='21_confidence-intervals_cache/revealjs/precip.bootstrap.ci_989c066a45dacc25e67c9b394db85ae4'}\n\n```{.r .cell-code}\nbootstrap_Xbar <- \n  replicate(10000, mean(sample(precip, size = length(precip), replace = TRUE)))\nquantile(bootstrap_Xbar, probs = c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%    97.5% \n31.76421 38.07000 \n```\n:::\n\n```{.r .cell-code}\nprecip_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31.61748 38.15395\nattr(,\"conf.level\")\n[1] 0.95\n```\n:::\n:::\n\n\n. . .\n\nIn this case, since $n$ is large, the non-normality is not a big deal.\n\n## Confidence intervals for proportions\n\nFor large $n$, the following confidence interval is often used for a population proportion:\n\n$$\\widehat{p}\\pm z_{\\alpha/2}\\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}$$\n\nNote that this formula only gives an approximate CI because of the use of $\\widehat{p}$ when estimating the standard error of $\\widehat{p}$.\n\n## Other confidence intervals\n\nR has implementations for confidence intervals for many types of population parameters, including for non-normally distributed populations. Always carefully read the documentation before using these functions.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}