---
title: "MATH167R: Simulations"
author: "Peter Gao"
format: 
  revealjs:
    theme: [./slides.scss, ../theme.scss]
editor: visual
---

## Overview of today

-   Random walks continued
-   Timing code

## Warm up: Estimating $\pi$

The area of a radius $r$ circle is $\pi r^2$. How could we use simulation to estimate $\pi$?

. . .

Hint: what if we randomly generated points on a square?

. . .

```{r}
#| echo: TRUE
#| eval: FALSE
#| message: FALSE
#| warning: FALSE
library(tidyverse)
library(ggplot2)
random_points <- data.frame(x = runif(500, -1, 1),
                            y = runif(500, -1, 1))
random_points <- random_points |>
  mutate(in_circle = x^2 + y^2 < 1)
ggplot(random_points, aes(x = x, y = y, color = in_circle)) + 
  geom_point() +
  theme(aspect.ratio=1)
```

## Warm up: Estimating $\pi$

```{r random_points_1}
#| echo: FALSE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
library(tidyverse)
library(ggplot2)
random_points <- data.frame(x = runif(500, -1, 1),
                            y = runif(500, -1, 1))
random_points <- random_points |>
  mutate(in_circle = x^2 + y^2 < 1)
ggplot(random_points, aes(x = x, y = y, color = in_circle)) + 
  geom_point() +
  theme(aspect.ratio=1)
```

## Warm up: Estimating $\pi$

```{r random_points}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
random_points <- data.frame(x = runif(500, -1, 1),
                            y = runif(500, -1, 1))
random_points <- random_points |>
  mutate(in_circle = x^2 + y^2 < 1)
random_points |> summarize(pi_est = 4 * mean(in_circle))
```

## Random walks continued

Recall the following code for a one-dimensional random walk:

```{r random_walk_functional}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
take_step <- function() {
  # move forwards or backwards with equal probability
  return(sample(c(1, -1), 1))
}
walk_randomly <- function(n_steps, start = 0) {
  if (n_steps <= 1) {
    return(start)
  }
  x <- c(start, walk_randomly(n_steps - 1, start + take_step()))
  return(x)
}
plot(1:100, walk_randomly(100), type = "l",
     xlab = "Step", ylab = "x",
     main = "A one-dimensional random walk")
```

## Random walks continued

We can use this simulation code to estimate a number of properties about random walks, including:

1.  If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?

2.  If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?

3.  The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?

4.  What is the expected maximum value of $|x_i|$ for a random walk of length $n$?

## Random walks continued

*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*

Consider that the results of `walk_randomly(n_steps)` is a vector of length `n_steps`.

```{r}
#| echo: TRUE
#| eval: TRUE
set.seed(123)
walk_randomly(20)
```

. . .

We need an expression to count the number of times we reach $x=0$ (minus the start).

```{r}
#| echo: TRUE
#| eval: TRUE
set.seed(123)
sum(walk_randomly(20) == 0) - 1
```

## Random walks continued

*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*

Now we can use `replicate()` to repeatedly evaluate our expression and then take the mean:

```{r}
#| echo: TRUE
#| eval: TRUE
set.seed(123)
mean(replicate(10000, sum(walk_randomly(20) == 0) - 1))
```

## Varying `n_steps` parameter

*If we consider a random walk of length $n$, how often will the walker visit $x=0$, on average?*

```{r varying_n_steps}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
set.seed(123)
ns <- c(10, 20, 50, 100, 200, 500, 1000) 
expected_returns <- 
  vapply(ns, 
         function(n_steps) 
           mean(replicate(1000, sum(walk_randomly(n_steps) == 0) - 1)),
         numeric(1))
plot(ns, expected_returns, type = "l", 
     xlab = "Number of steps", 
     ylab = "Mean returns to x = 0")
```

## Random walks continued

How could we adapt this code to answer the question:

*If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?*

As $n\rightarrow\infty$, how will the expected number of visits to $x=10$ before step $n$ change?

## Random walks continued

*If we consider a random walk of length $n$, how often will the walker visit $x=10$, on average?*

```{r varying_n_steps_10}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
set.seed(123)
ns <- c(10, 20, 50, 100, 200, 500, 1000) 
expected_returns <- 
  vapply(ns, 
         function(n_steps) 
           mean(replicate(1000, sum(walk_randomly(n_steps) == 10))),
         numeric(1))
plot(ns, expected_returns, type = "l", 
     xlab = "Number of steps", 
     ylab = "Mean visits to x = 10")
```



## Expected time of return

Can we adapt this code to answer the following question?

*The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?*

We have to be a little clever here--we only ever compute a finite-length random walk.

For any nonnegative integer valued random variable $X$,

$$E(X)=\sum_{n = 1}^\infty P(X>n)$$

## Expected time of return

```{r expected_return}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
set.seed(123)
ns <- 1:20
# for each n, estimate the proportion of random walks that do NOT return
# to zero, using simulations
proportion_no_returns <- 
  vapply(ns, 
         function(n_steps) 
           mean(replicate(2500, sum(walk_randomly(n_steps) == 0) < 2)),
         numeric(1))
plot(ns, proportion_no_returns, type = "l", 
     xlab = "Number of steps", 
     ylab = "Mean visits to x = 10")
proportion_no_returns
```

## Expected time of return

Does the infinite series converge?

```{r partial_sums}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
# calculate partial sums of the infinite series
cumsum(proportion_no_returns)
```

## Expected time of return

*The walker starts at $x=0$. What is the expected length of time before the walker returns to $x=0$ (if it is finite)?*

It turns out the expected length of time is infinite--a key example of why we have to be careful about using simulations to replace mathematical analysis.

## Exercise

How would we write a simulation to answer the following question:

*What is the expected maximum value of $|x_i|$ for a random walk of length $n$?*

## Timing code

When running simulations, the number of different dimensions we consider (number of simulations, number of steps, etc.) can rapidly increase computational cost.

We often may want to be able to estimate the runtime of our simulations or to compare the speed of various versions of a simulation.

**Example**: Which is faster: vectorized code or a for loop?

```{r example_vec}
#| echo: TRUE
#| eval: FALSE
#| cache: TRUE

# vectorized
x_vec <- rnorm(100000) + 1

# for loop
x_for <- numeric()
for (i in 1:100000){
  x_for[i] <- rnorm(1) + 1
}

```

## `Sys.time()`

The `Sys.time()` function can be used to report the time at evaluation.

```{r systime}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
start_time <- Sys.time()
x_vec <- rnorm(100000) + 1
print(Sys.time() - start_time)
start_time <- Sys.time()
x_for <- numeric()
for (i in 1:100000){
  x_for[i] <- rnorm(1) + 1
}
print(Sys.time() - start_time)
```

## `system.time()`

Altenatively, the `system.time()` function can be used to time the evaluation of a specific expression,

```{r systemtime}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
system.time(x_vec <- rnorm(100000) + 1)

x_for <- numeric()
system.time(

  for (i in 1:100000){
    x_for[i] <- rnorm(1) + 1
  }
)
```

## `microbenchmark`

Finally, the `microbenchmark` package can be used to time functions repeatedly.

```{r microbenchmark}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
# vectorized
sq_vectorized <- function(x) {
    return(x ^ 2)
}
# for loop
sq_for_loop <- function(x) {
    for (i in 1:length(x)) {
        x[i] <- x[i] ^ 2
    }
    return(x)
}

library(microbenchmark)
# microbenchmark to compare the two functions
x <- rnorm(1000)
compare_sq <- microbenchmark(sq_vectorized(x),
                             sq_for_loop(x),
                             times = 100)
compare_sq
```

## Algorithmic efficiency

The amount of time code takes is related to the efficiency of the underlying algorithm.

Algorithms are commonly analyzed in terms of their asymptotic complexity. The most common way to describe complexity is Big O notation, where an algorithm with $O(g(n))$ complexity has a time requirement which is asymptotically proportional to $g(n)$.

Examples:

-   Finding the median in a sorted list of numbers is $O(1)$ constant complexity.

-   Finding the largest number in an unsorted list of numbers is $O(n)$ linear complexity.

In practice, timing code can be used to examine these relationships

## Example: inverting matrices.

Matrix inversion is typically said to be $O(n^3)$.

```{r inversion}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
compare_mat <- microbenchmark(mat_3_x_3 = solve(matrix(rnorm(9), nrow = 3)),
                              mat_5_x_5 = solve(matrix(rnorm(25), nrow = 5)),
                              mat_7_x_7 = solve(matrix(rnorm(49), nrow = 7)),
                              mat_9_x_9 = solve(matrix(rnorm(81), nrow = 9)),
                              times = 100)
compare_mat
```
