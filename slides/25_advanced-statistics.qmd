---
title: "MATH167R: Advanced statistical methods"
author: "Peter Gao"
format: 
  revealjs:
    theme: [./slides.scss, ../theme.scss]
editor: visual
---

## Overview of today

-   ANOVA
-   The $F$-test for Regression Analysis
-   Logistic regression

## Analysis of Variance

The analysis of variance or ANOVA is a collection of statistical methods used to compare two or more groups to assess the differences between group means.

The simplest ANOVA provides a test of whether two or more population means are equivalent. As such the ANOVA is a generalization of the two sample $t$-test.

ANOVA is typically said to be introduced by Ronald Fisher, who applied the method to study the variation in crop yield related to different fertiliser treatments.

## One-way ANOVA

One-way ANOVA or single-factor ANOVA is used to compare two or more population means. Let $I$ be the number of groups/populations and $\mu_I$ to be the population mean of the response of interest for group $I$.

**Hypotheses**:

-   $H_0:\mu_1=\ldots=\mu_I$
-   $H_a: \text{at least two of the means are different}$

**Assumptions**:

-   The population distributions are all normal with the same variance $\sigma^2$.

## Test Statistic

The $F$ test is commonly used to compare the causes of the total variation. In particular, the $F$ test statistic is defined as

$$F=\frac{\text{variance between groups}}{\text{variance within groups}}$$

If $F$ is large, then there may be evidence to reject the hypothesis that the group means are all equal.

## Test Statistic

$$F=\frac{\text{variance between groups}}{\text{variance within groups}}=\frac{
\text{Mean Square for Treatments}}{\text{Mean Square for Error}}$$

In a balanced design, where each group has the same number of $J$ samples, and $X_{i, j}$ denotes the $j$th measurement from group $i$, the mean square terms have the following formulas:

$$\text{Mean Square for Treatments}=\frac{J}{I-1}\sum_i(\overline{X}_i-\overline{X}_{\cdot\cdot})^2$$

where $\overline{X}_i$ is the sample group mean and $\overline{X}_{\cdot\cdot}$ is the overall sample mean.

$$\text{Mean Square for Error}=\frac{S_1^2+S_2^2+\cdots+S_I^2}{I}$$

## Example: `PlantGrowth`

The `PlantGrowth` data provides results from an experiment comparing plant growth under a control and two different treatment conditions.

```{r}
#| echo: TRUE
#| eval: TRUE
boxplot(weight ~ group, data = PlantGrowth, main = "PlantGrowth data",
        ylab = "Dried weight of plants", col = "lightgray")
```

## Example: `PlantGrowth`

```{r}
#| echo: TRUE
#| eval: TRUE
anova_res <- anova(lm(weight ~ group, data = PlantGrowth))
anova_res
```

## Obtaining a $p$-value with an $F$-test statistic

When $H_0$ is true, $F$ follows an $F$-distribution with $\nu_1=I-1$ and $\nu_2=I(J-1)$.

```{r}
#| echo: TRUE
#| eval: TRUE
#| fig-align: "center"
library(ggplot2)
x <- data.frame(x = seq(0, 8, length.out = 3))
ggplot(x, aes(x = x)) + 
  geom_function(fun = df, args = list(df1 = 2, df2 = 27)) + 
  geom_vline(xintercept = anova_res$`F value`, color = "red") +
  theme_minimal()
```

## The $F$-distribution

The $F$ distribution has two parameters:

-   numerator degrees of freedom $\nu_1$
-   denominator degrees of freedom $\nu_2$

The $F$ distribution was derived as the ratio of (scaled) chi-square random variables and is frequently used to model the null distribution of test statistics.

$$F = \frac{U_1/d_1}{U_2/d_2}$$

where $U_1$ is chi-square with $d_1$ degrees of freedom, $U_2$ is chi-square with $d_2$ degrees of freedom, and $U_1$ and $U_2$ are independent.

## Example: `emails`

The `emails` data provides information on emails categorized as spam and non-spam.

```{r}
#| echo: TRUE
#| eval: TRUE
emails <- read.csv("https://www.openintro.org/data/csv/email50.csv")
boxplot(num_char ~ spam, data = emails, main = "Emails data",
        ylab = "Length of email (characters)", col = "lightgray")
```

## Example: `emails`

```{r}
#| echo: TRUE
#| eval: TRUE
anova_res <- anova(lm(num_char ~ spam, data = emails))
anova_res
```

## The General Linear $F$-test

In general, the `anova()` function can be used to test the fit of a model for sample data.

For a simple linear regression case, the $F$ test can be used to compare a **full** model of the form

$$y_i = (\beta_0+\beta_1x_{i1})+\epsilon_i$$

with a **reduced** model:

$$y_i=\beta_0+\epsilon_i$$

**Hypotheses**:

-   $H_0:y_i=\beta_0+\epsilon_i$
-   $H_a: y_i=\beta_0+\beta_1x_{i1}+\epsilon_i$

## The General Linear $F$-test

Let $p_{full}$ be the number of parameters in the full model and $p_{reduced}$ be the number of parameters in the reduced model.

Let $SSE_{full}$ and $SSE_{reduced}$ be the sum of squared errors for the full and reduced models, respectively.

$$
F = \frac{(SSE_{reduced}-SSE_{full})/(p_{full}-p_{reduced})}{SSE_{full}/p_{full}}
$$

Then $F$ follows an $F$ distribution with $p_{full}-p_{reduced}$ and $p_{full}$ degrees of freedom.

## Example: `mtcars`

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
my_lm <- lm(mpg ~ wt, data = mtcars)
anova(my_lm)
```

## Example: `mtcars`

Note that this is equivalent to the Wald test for the coefficient for `wt` in the univariate case:

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
summary(my_lm)
```

## The General Linear $F$-test

More generally the $F$-test can be used to test sets of nested models, when a reduced model can be compared with a full model.

**Hypotheses**:

-   $H_0: \text{reduced model true}$
-   $H_a: \text{full model fits better}$

## Example: `mtcars`

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
my_lm <- lm(mpg ~ hp + wt, data = mtcars)
anova(my_lm)
```

When we have more than one explanatory variable, the $F$-test is different from the univariate Wald test.

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
summary(my_lm)
```

## Example: `mtcars`

We can explicitly specify the full and reduced models:

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
my_lm_full <- lm(mpg ~ hp + wt, data = mtcars)
my_lm_reduced <- lm(mpg ~ 1, data = mtcars)
anova(my_lm_full, my_lm_reduced)
```

## Logistic regression

When we have binary outcomes $Y_1,\ldots, Y_n$ where $Y_i\in \{0, 1\}$ for all $i$, it may be inappropriate to use linear regression to model the relationships between $Y$ and covariates $X$.

**Logistic regression** can be used to relate the **probability** that $Y_i=1$ to available covariates.

## Logistic regression

**Model:**

$$P(Y_i=1)=p_i$$ $$\text{logit}(p_i)=\log\left(\frac{p_i}{1-p_i}\right)=\mathbf{x}_i^\top\boldsymbol\beta$$ where $\mathbf{X}_i$ is the covariate vector for individual $i$ and $\boldsymbol\beta$ is a vector of regression covariates.

Note that the logit transformation has domain $(0,1)$ and range $(-\infty, \infty)$.

## Logistic regression

When we have only one covariate $x$, the model is as follows:

$$\text{logit}(p(x))=\log\left(\frac{p(x)}{1-p(x)}\right)=\beta_0+\beta_1 x$$

where $p(x)$ represents the probability that an individual with covariate value $x$ experiences the response/event of interest.

## Example: *Challenger* disaster

On January 28, 1986, the Challenger space shuttle suffered an accident shortly after launch, killing all crew members on board. An investigation suspected that damage to a set of critical parts called O-rings caused the disaster. It is believed that the damage is related to the temperature at launch time. The following data summarize data on O-rings for 23 shuttle missions.

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
orings <- readr::read_csv("https://www.openintro.org/data/csv/orings.csv")
head(orings)
```

## Example: *Challenger* disaster

This plot visualizes the empirical proportion of damaged O-rings for each mission:

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
plot((damaged / 6) ~ temperature, data = orings, pch = 16,
     ylab = "Proportion of O-rings damaged", xlab = "Temperature (F)")
```

## Example: *Challenger* disaster

The `glm()` function can be used to fit generalized linear models including logistic regression models. The `family` argument is used to specify the distribution of the response variable (in this case, binomial for a logistic rgression).

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
orings_res <- glm((damaged > 0) ~ temperature, data = orings, family = binomial)
summary(orings_res)
```

## Example: *Challenger* disaster

Here, our estimate of $\beta_0$ is $b_0=15.04$ and our estimate of $\beta_1$ is $b_1=-.23$.

We can interpret $\beta_1$ as follows: for each additional degree of ambient temperature, the log-odds of experiencing an O-ring failure decreases by $.23$.

In other words, the odds changes by a factor of $\exp(.23)\approx.79$ as temperature increases by one degree.

Note that the $p$-value for $\beta_1$ is significant at the $.05$ level, suggesting evidence that temperature is associated with O-ring failure.

## Example: *Challenger* disaster

We can use many of the functions we used with `lm` objects, including:

-   `fitted()` returns the predicted probabilities for a logistic regression
-   `predict()` can be used to generate predictions
-   `plot()` returns a series of diagnostic plots
-   `residuals()` returns residuals--not that these are not usually the typical observed minus predicted residuals.
-   `anova()` can be used to assess the model fit for `glm` objects.

## Example: *Challenger* disaster

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
fitted(orings_res)
```

## Example: *Challenger* disaster

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
#| message: FALSE
#| warning: FALSE
base_res <- glm((damaged > 0) ~ 1, data = orings, family = binomial)
anova(orings_res, base_res, test = "F")
```

# Other hypothesis tests

## Fisher's exact test

*Agresti (1990): A British woman claimed to be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first. The null hypothesis is that there is no association between the true order of pouring and the woman's guess. The alternative that there is a positive association (that the odds ratio is greater than 1).*

## Fisher's exact test

```{r}
#| echo: TRUE
#| eval: TRUE
#| cache: TRUE
TeaTasting <-
  matrix(c(3, 1, 1, 3),
         nrow = 2,
         dimnames = list(Guess = c("Milk", "Tea"),
                         Truth = c("Milk", "Tea")))
fisher.test(TeaTasting, alternative = "greater")
```
